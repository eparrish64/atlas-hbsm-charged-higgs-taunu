#!/usr/bin/env python2.7

## stdl
import sys, os, re, gc, glob, argparse
import cPickle, dill

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_evalclf_parser 
eclf_parser = get_evalclf_parser()


ECLF_ARGS = eclf_parser.parse_args()


## local
from hpana.variables import CLF_FEATURES, BDT_SCORES
from hpana.dataset_hists import dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration
from hpana.mva.classifier import Classifier, SClassifier
from hpana.mva import N_TRACKS, PKL_FILE_PATTERN, XML_FILE_PATTERN, ALL_FEATS, BRANCHES 
from hpana.mva.evaluation import AppendJob, plot_scores, get_models, evaluate_scores_on_trees

## setup logger
from hpana import log
log.setLevel(ECLF_ARGS.log)
 
## Setup ROOT
import ROOT
from ROOT import TMVA
ROOT.gROOT.SetBatch(True)


#-----------------------------------------------
# consts
#-----------------------------------------------
TMP_DATASETS = [ 
    "LepFakes.Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "LepFakes.Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV280_500_CVetoBVeto_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV1000_E_CMS_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV500_1000_9364",
    "Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
]

__HERE = os.path.dirname(os.path.abspath(__file__))

COPY_FILE = True
USE_FASTBDT = False
kFOLDS = ECLF_ARGS.kfolds
METHOD_TYPE = TMVA.Types.kBDT #<! TMVA method 

if ECLF_ARGS.backend=="tmva":
    ## initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
if ECLF_ARGS.mtype=="keras":
    METHOD_TYPE = TMVA.Types.kPyKeras
    ## - -  Select Theano as backend for Keras
    os.environ['KERAS_BACKEND'] = 'theano'
    
    ## - - Set architecture of system (AVX instruction set is not supported on SWAN)
    environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'


##-----------------------------------------------
## main driver
##-----------------------------------------------
def main():
    import time
    itime = time.time()
    if not ECLF_ARGS.models:
        raise IOError('Path to trained models pls ?')        

    ## - - setup outdir
    os.system("mkdir -p %s"%ECLF_ARGS.outdir)

    if ECLF_ARGS.plot_scores:
        models = get_models(ECLF_ARGS.models)
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)
        
        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        ## - - prepare testing dataFrame 
        backgrounds =  filter(lambda b: not b.name in ["LepFakes"], analysis.backgrounds)
        if ECLF_ARGS.bkg:
            backgrounds = filter(lambda b: b.name in ECLF_ARGS.bkg, backgrounds)

        signals = analysis.get_signals()
        if ECLF_ARGS.sig:
            signals = filter(lambda s: s.name in ECLF_ARGS.sig, signals)
        if ECLF_ARGS.mass_range:
            signals = filter(lambda s:int(ECLF_ARGS.mass_range[0]) <= s.mass <= int(ECLF_ARGS.mass_range[1]), signals)   
    

        dframe = SClassifier.prepare_data(backgrounds, signals, ALL_FEATS[ECLF_ARGS.channel], data_lumi=config.data_lumi,
                                      channel=ECLF_ARGS.channel, branches=BRANCHES[ECLF_ARGS.channel], train_data=ECLF_ARGS.train_data)

        jobs = []
        for mtag, trained_models in models.iteritems():
            jobs += [Job(plot_scores, trained_models, 
                dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=ECLF_ARGS.outdir, formats=[".pdf", ".png"], inclusive_trks=ECLF_ARGS.inc_trks)]

        if ECLF_ARGS.parallel:
            log.info("************** submitting %i jobs  ************" % len(jobs))
            log.info("***********************************************")
            run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)
        else:
            run_pool(jobs, n_jobs=1)

    if ECLF_ARGS.direct:
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # # - -  some checks on cmd args
        if ECLF_ARGS.fields:
            fields = filter(lambda v: v.name in ECLF_ARGS.fields, BDT_SCORES[ECLF_ARGS.channel])
        else:
            fields = BDT_SCORES[ECLF_ARGS.channel]

        if ECLF_ARGS.categories:
            categories = filter(
                lambda c: c.name in ECLF_ARGS.categories, config.categories)
        else:
            categories = config.categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if ECLF_ARGS.systematics:
            systematics = filter(
                lambda s: s.name in ECLF_ARGS.systematics, all_systematics)
        elif ECLF_ARGS.systs:
            systematics = []# all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # - -  if you wish to look at specific samples
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        ## multicore processing 
        if ECLF_ARGS.parallel:
            models = get_models(ECLF_ARGS.models)
            ## prepare hist workers 
            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()

            jobs = []
            for w in ana_workers:
                jobs.append(Job(dataset_hists_direct, w, clf_models=models, write_hists=True, outdir=ECLF_ARGS.outdir))

            log.info("************** submitting %i jobs  ************" % len(jobs))
            log.info("***********************************************")
            run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)

        ## submit to the batch system 
        if ECLF_ARGS.cluster:
            ## retry failed jobs ?
            if ECLF_ARGS.retry:
                # submitted_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/submitted*"%ECLF_ARGS.outdir)])
                submitted_jobs = set([j.split("/")[-1].replace(".slurm", "") for j in glob.glob("%s/jobs/*.slurm"%ECLF_ARGS.outdir)])
                completed_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/done*"%ECLF_ARGS.outdir)])
        
                failed_jobs = []
                for j in submitted_jobs:
                    if not j in completed_jobs:
                            if not j in failed_jobs:
                                failed_jobs += [j]

                log.warning("Following %i jobs are failed; you have to resubmit them"%len(failed_jobs))
                submit_file = open("%s/submit_%i_failed.sh"%(ECLF_ARGS.outdir, len(failed_jobs)), "w")

                for j in failed_jobs:
                    log.info("updating job script for %s"%j) 
                    submit_file.write("sbatch %s/jobs/%s.slurm \n"%(ECLF_ARGS.outdir, j))
                submit_file.close()
                return 
                
            ## setup the submit dir
            os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists".format(ECLF_ARGS.outdir))
            submitfile_name = "%s/%s.sh" % (ECLF_ARGS.outdir, "submitAllJobs")
            submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()
            ## pickle the analysis: IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
            with open(ECLF_ARGS.pickled_analysis, "wb") as pfile:
                dill.dump(analysis, pfile)

            if ECLF_ARGS.rs_manager=="SLURM":
                from hpana.cluster.job_template import SLURM_JOB_TEMPLATE

                for aw in ana_workers:
                    if ECLF_ARGS.dry_run:
                        print "--"*100
                        print aw.name
                        continue
                    # if not ".".join(aw.name.split(".")[:-1]) in TMP_DATASETS:
                    #     continue
                        
                    ## payload for container
                    payload = "source {job_script} {script_path} {pickled_analysis} {models}".format(
                        job_script=os.path.join(__HERE, "slurm_job_evalclf.sh"),
                        script_path=os.path.join(__HERE, "classifier-evaluator"),
                        pickled_analysis=os.path.abspath(ECLF_ARGS.pickled_analysis),
                        models=",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                    )

                    ## write the job submission script with right permissions
                    jfile_name = "%s/jobs/%s.slurm" % (ECLF_ARGS.outdir, aw.name)
                    jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                    os.write(jfile,
                            SLURM_JOB_TEMPLATE.format(
                                logsdir=os.path.join(
                                    ECLF_ARGS.outdir, ECLF_ARGS.logsdir),
                                outdir=ECLF_ARGS.outdir,
                                local_scratch=ECLF_ARGS.local_scratch,
                                cores=1,
                                time="4:0:0",
                                memory="3GB",
                                jobname=aw.name,  #<! important - jobname steers which worker used from pickled ana
                                payload=payload,
                                project=ECLF_ARGS.rs_project
                            )
                            )

                    os.close(jfile)

                    ## write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                            (submitfile_name, aw.name))
                    os.write(submit_file, "sbatch %s\n" % jfile_name)

                os.close(submit_file)
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t sh {0}".format(submitfile_name))        

            elif ECLF_ARGS.rs_manager == "CONDOR":
                from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

                ## zip source code
                log.info("Creating source code tarball...")
                source_code_tarball = os.path.abspath(ECLF_ARGS.outdir+"/source_code.tar.gz")
                if os.path.isfile(source_code_tarball):
                    os.system("rm -rf %s"%source_code_tarball)

                src_ds = ["bin", "hpana", "aux", "setup.sh", ECLF_ARGS.pickled_analysis]
                src_ds = " ".join(src_ds)
                os.system("cd {src_dir} && tar -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar}".format(
                    src_dir=__HERE+"/../", target_tar=source_code_tarball, source_files=src_ds))

                # - - setup the submit dir
                os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(ECLF_ARGS.outdir))



                if ECLF_ARGS.bin_scheme == "SINGLE":
                    from hpana.mva import TRAINING_MASS_BINS
                    indivmodels = {"%sto%s"%(i[0],i[0]): [] for i in TRAINING_MASS_BINS["SINGLE"]}
                    for mass_point in indivmodels.keys():
                        for model in ECLF_ARGS.models:
                            if mass_point in os.path.basename(model):
                                indivmodels[mass_point].append(model)
                            else:
                                continue

                    # log.info("************** creating %i jobs for HTCondor submission ************" % len(sum(len(v)for v in indivmodels.values())))
                    # log.info("**************************************************************")

                    for mass_point in indivmodels:

                        submit_file_name = "%s/submitAllJobs%s.sh" % (ECLF_ARGS.outdir, mass_point)
                        submit_file = open(submit_file_name, "w")
                        submit_file.write(
                                         CONDOR_JOB_TEMPLATE.format(
                                             logsdir=os.path.join(
                                                 ECLF_ARGS.outdir, mass_point, ECLF_ARGS.logsdir),
                                             execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                             userEmail="Z1832314@students.niu.edu",
                                             memory="3GB",
                                         ))

                        os.system("mkdir -p {0}/{1}/jobs  {0}/{1}/logs {0}/{1}/hists {0}/{1}/logs/log {0}/{1}/logs/err {0}/{1}/logs/out".format(ECLF_ARGS.outdir, mass_point))
                        for aw in ana_workers:

                            submit_file.write("\nArguments = %s %s %s %s %s %s %s\nqueue\n" %(aw.name, source_code_tarball, os.path.join(__HERE, "classifier-evaluator"), 
                                                                                        os.path.join(os.getcwd(), ECLF_ARGS.outdir,mass_point), 
                                                                                        os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                        os.path.abspath(ECLF_ARGS.models_path[0]), 
                                                                                        ",".join(os.path.basename(m) for m in indivmodels[mass_point])))



                        submit_file.close()
                        log.warning("Submission not supported from within container.")
                        log.warning("Please launch outside container via:")
                        log.warning("\t condor_submit {0}".format(submit_file_name)) 


                else:

                    submit_file_name = "%s/submitAllJobs.sh" % (ECLF_ARGS.outdir)
                    submit_file = open(submit_file_name, "w")
                    submit_file.write(
                                     CONDOR_JOB_TEMPLATE.format(
                                         logsdir=os.path.join(
                                             ECLF_ARGS.outdir, ECLF_ARGS.logsdir),
                                         execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                         userEmail="Z1832314@students.niu.edu",
                                         memory="3GB",
                                     ))
                    log.info("************** creating %i jobs for HTCondor submission ************" % len(ECLF_ARGS.models))
                    log.info("**************************************************************")
                    

                    for aw in ana_workers:

                        submit_file.write("\nArguments = %s %s %s %s %s %s %s\nqueue\n" %(aw.name, source_code_tarball, os.path.join(__HERE, "classifier-evaluator"), 
                                                                                        os.path.join(os.getcwd(), ECLF_ARGS.outdir), 
                                                                                        os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                        os.path.abspath(ECLF_ARGS.models_path[0]), 
                                                                                        ",".join([os.path.basename(m) for m in ECLF_ARGS.models])))



                        # - - - - write submission to file to be used outside container
                        log.info("Writing to %s for job %s ..." %
                                 (submit_file_name, aw.name))
                        # os.write(submit_file, "sbatch %s\n" % jfile_name)

                    submit_file.close()
                    log.warning("Submission not supported from within container.")
                    log.warning("Please launch outside container via:")
                    log.warning("\t condor_submit {0}".format(submit_file_name)) 


    if ECLF_ARGS.merge_hists:
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()
        log.info(
            "************** merging histograms  ************")
        log.info(
            "***********************************************")
        analysis.merge_hists(samples=samples, histsdir=ECLF_ARGS.outdir, overwrite=True, write=True)


    if ECLF_ARGS.append:
        models = get_models(ECLF_ARGS.models)
        ## sort files based on size to start the heavier ones sooner.
        ECLF_ARGS.files.sort(key=lambda f: os.path.getsize(f), reverse=True)
        jobs = [AppendJob(f, models, copy_file=COPY_FILE, outdir=ECLF_ARGS.outdir) for f in ECLF_ARGS.files]

        st = time.time()
        ## run a pool of jobs
        if ECLF_ARGS.parallel or len(jobs)> 1:
            run_pool(jobs, n_jobs=-1)
        else:
            ## processing one file only (also for PBS, CONDOR Batch)
            for job in jobs:
                job.run()

    ftime = time.time()
    elapsed_time = (ftime - itime)
    log.info("\n****************** elapsed time: %i sec ******************" % elapsed_time)


if __name__=='__main__':
    main()