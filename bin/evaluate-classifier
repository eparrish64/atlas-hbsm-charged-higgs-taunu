#!/usr/bin/env python2.7

## stdl
import sys, os, re, gc, glob, argparse
import cPickle

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_clf_parser 
clf_parser = get_clf_parser()

clf_parser.add_argument('--files', nargs='+', 
                    help='ntuples to append clf score to them')

clf_parser.add_argument('--models', '-w', nargs="+",
                        help='path to xml or pickled trained models')

clf_parser.add_argument("--mtype", choices=["keras", "bdt"], default="bdt",
                        help="TMVA Method type")

clf_parser.add_argument("--fields", nargs="+",
                    help="list of the variables that you want to analyze")

clf_parser.add_argument("--categories", nargs="+",
                    help="list of the categories that you want to analyze")

clf_parser.add_argument("--systematics", nargs="+",
                    help="list of the systematics that you want to analyze")

clf_parser.add_argument("--systs", "-s", action="store_true",
                    help="process all systematics")

clf_parser.add_argument("--samples", nargs="+",
                            help="list of samples to process")

clf_parser.add_argument("--merge-hists", action="store_true",
                            help="merge histograms")

clf_parser.add_argument("--append", action="store_true",
                            help="append clf scores to TTrees")

clf_parser.add_argument("--direct", action="store_true",
                            help="calculate clf scores on the fly")

clf_parser.add_argument("--plot-scores", action="store_true",
                            help="calculate and plot clf scores directly from np arrays")

CLF_ARGS = clf_parser.parse_args()


## local
from hpana.variables import CLF_FEATURES, BDT_SCORES
from hpana.dataset_hists import dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration
from hpana.mva.classifier import Classifier, SClassifier
from hpana.mva import N_TRACKS, PKL_FILE_PATTERN, XML_FILE_PATTERN, ALL_FEATS, BRANCHES 
from hpana.mva.evaluation import AppendJob, plot_scores, get_models, evaluate_scores_on_trees

## setup logger
from hpana import log
log.setLevel(CLF_ARGS.log)
 
## Setup ROOT
import ROOT
from ROOT import TMVA
ROOT.gROOT.SetBatch(True)


#-----------------------------------------------
# consts
#-----------------------------------------------
COPY_FILE = True
USE_FASTBDT = False
kFOLDS = CLF_ARGS.kfolds
METHOD_TYPE = TMVA.Types.kBDT #<! TMVA method 

if CLF_ARGS.backend=="tmva":
    ## initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
if CLF_ARGS.mtype=="keras":
    METHOD_TYPE = TMVA.Types.kPyKeras
    ## - -  Select Theano as backend for Keras
    os.environ['KERAS_BACKEND'] = 'theano'
    
    ## - - Set architecture of system (AVX instruction set is not supported on SWAN)
    environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'


##-----------------------------------------------
## main driver
##-----------------------------------------------
if __name__=='__main__':
    import time
    itime = time.time()
    if not CLF_ARGS.models:
        raise IOError('Path to trained models pls ?')        
    models = get_models(CLF_ARGS.models)

    ## - - setup outdir
    os.system("mkdir -p %s"%CLF_ARGS.outdir)

    if CLF_ARGS.plot_scores:
        # - -  build analysis main configuration object
        config = Configuration(CLF_ARGS.channel, data_streams=CLF_ARGS.data_streams, db_version=CLF_ARGS.db_version)
        
        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        ## - - prepare testing dataFrame 
        backgrounds =  filter(lambda b: not b.name in ["LepFakes"], analysis.backgrounds)
        if CLF_ARGS.bkg:
            backgrounds = filter(lambda b: b.name in CLF_ARGS.bkg, backgrounds)

        signals = analysis.get_signals()
        if CLF_ARGS.sig:
            signals = filter(lambda s: s.name in CLF_ARGS.sig, signals)
        if CLF_ARGS.mass_range:
            signals = filter(lambda s:int(CLF_ARGS.mass_range[0]) <= s.mass <= int(CLF_ARGS.mass_range[1]), signals)   
    
        dframe = SClassifier.prepare_data(backgrounds, signals, ALL_FEATS[CLF_ARGS.channel], data_lumi=config.data_lumi,
                                      channel=CLF_ARGS.channel, branches=BRANCHES[CLF_ARGS.channel], train_data=CLF_ARGS.train_data)

        jobs = []
        for mtag, trained_models in models.iteritems():
            jobs += [Job(plot_scores, trained_models, dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=CLF_ARGS.outdir)]

        if CLF_ARGS.parallel:
            log.info("************** submitting %i jobs  ************" % len(jobs))
            log.info("***********************************************")
            run_pool(jobs, n_jobs=CLF_ARGS.ncpu)
        else:
            run_pool(jobs, n_jobs=1)

    if CLF_ARGS.direct:
        # - -  build analysis main configuration object
        config = Configuration(CLF_ARGS.channel, data_streams=CLF_ARGS.data_streams, db_version=CLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # # - -  some checks on cmd args
        if CLF_ARGS.fields:
            fields = filter(lambda v: v.name in CLF_ARGS.fields, BDT_SCORES[CLF_ARGS.channel])
        else:
            fields = BDT_SCORES[CLF_ARGS.channel]

        if CLF_ARGS.categories:
            categories = filter(
                lambda c: c.name in CLF_ARGS.categories, config.categories)
        else:
            categories = config.categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if CLF_ARGS.systematics:
            systematics = filter(
                lambda s: s.name in CLF_ARGS.systematics, all_systematics)
        elif CLF_ARGS.systs:
            systematics = []# all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # - -  if you wish to look at specific samples
        if CLF_ARGS.samples:
            samples = filter(lambda s: s.name in CLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        if CLF_ARGS.parallel:
            ## prepare hist workers 
            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()

            jobs = []
            for w in ana_workers:
                jobs.append(Job(dataset_hists_direct, w, clf_models=models, write_hists=True, outdir=CLF_ARGS.outdir))

            log.info("************** submitting %i jobs  ************" % len(jobs))
            log.info("***********************************************")
            run_pool(jobs, n_jobs=CLF_ARGS.ncpu)
        
        if CLF_ARGS.merge_hists:
            log.info(
                "************** merging histograms  ************")
            log.info(
                "***********************************************")
            analysis.merge_hists(samples=samples, histsdir=CLF_ARGS.outdir, overwrite=True, write=True)

    if CLF_ARGS.append:
        ## sort files based on size to start the heavier ones sooner.
        CLF_ARGS.files.sort(key=lambda f: os.path.getsize(f), reverse=True)
        jobs = [AppendJob(f, models, copy_file=COPY_FILE, outdir=CLF_ARGS.outdir) for f in CLF_ARGS.files]

        st = time.time()
        ## run a pool of jobs
        if CLF_ARGS.parallel or len(jobs)> 1:
            run_pool(jobs, n_jobs=-1)
        else:
            ## processing one file only (also for PBS, CONDOR Batch)
            for job in jobs:
                job.run()

    ftime = time.time()
    elapsed_time = (ftime - itime)
    log.info("\n****************** elapsed time: %i sec ******************" % elapsed_time)

