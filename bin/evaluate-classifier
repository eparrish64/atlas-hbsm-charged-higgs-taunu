#!/usr/bin/env python2.7

## stdl
import sys, os, re, gc, glob, argparse
import cPickle, dill
from os import environ
import numpy as np
import pandas as pd

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_evalclf_parser 
eclf_parser = get_evalclf_parser()
ECLF_ARGS = eclf_parser.parse_args()
if not ECLF_ARGS.submitdir:
    ECLF_ARGS.submitdir = ECLF_ARGS.outdir
if not ECLF_ARGS.outdir:
    ECLF_ARGS.outdir = ECLF_ARGS.submitdir


## local
from hpana.variables import CLF_FEATURES, BDT_SCORES
from hpana.dataset_hists import dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration
from hpana.mva.classifier import Classifier, SClassifier
from hpana.mva import N_TRACKS, PKL_FILE_PATTERN, XML_FILE_PATTERN, ALL_FEATS, BRANCHES 
from hpana.mva.evaluation import AppendJob, plot_scores, get_models, evaluate_scores_on_trees

if ECLF_ARGS.eval_nn == True:
    environ['KERAS_BACKEND'] = 'tensorflow'
    # Set architecture of system (AVX instruction set is not supported on SWAN)
    environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'
    import tensorflow as tf
    import keras
    from keras.models import Sequential
    from keras.layers import Dense, Activation, BatchNormalization
    from keras.regularizers import l2
    from keras import initializers
    from keras.optimizers import SGD
    from keras.wrappers.scikit_learn import KerasClassifier
    from keras.models import load_model

## setup logger
from hpana import log
log.setLevel(ECLF_ARGS.log)
 
## Setup ROOT
import ROOT
from ROOT import TMVA
ROOT.gROOT.SetBatch(True)


#-----------------------------------------------
# consts
#-----------------------------------------------
TMP_DATASETS = [
    "LepFakes.Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "LepFakes.Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV280_500_CVetoBVeto_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV1000_E_CMS_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV500_1000_9364",
    "Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
]

__HERE = os.path.dirname(os.path.abspath(__file__))

COPY_FILE = False
USE_FASTBDT = False
kFOLDS = ECLF_ARGS.kfolds
METHOD_TYPE = TMVA.Types.kBDT #<! TMVA method 

if ECLF_ARGS.backend=="tmva":
    ## initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
# if ECLF_ARGS.mtype=="keras":
#     METHOD_TYPE = TMVA.Types.kPyKeras
#     ## - -  Select Theano as backend for Keras
#     os.environ['KERAS_BACKEND'] = 'theano'
    
#     ## - - Set architecture of system (AVX instruction set is not supported on SWAN)
#     environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'

def split(a, n):
    k, m = divmod(len(a), n)
    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))

def chunks(a, n):
    for i in xrange(0, len(a), n):
        yield a[i:i+n]

##-----------------------------------------------
## main driver
##-----------------------------------------------
def main():
    import time
    itime = time.time()
    if not ECLF_ARGS.models:
        raise IOError('Path to trained models pls ?')        

    ## - - setup outdir
    os.system("mkdir -p %s"%ECLF_ARGS.outdir)
    os.system("mkdir -p %s"%ECLF_ARGS.submitdir)

    if ECLF_ARGS.plot_scores:
        models, Keras_models = get_models(ECLF_ARGS.models, isNN=ECLF_ARGS.eval_nn)
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)
        
        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        ## - - prepare testing dataFrame 
        backgrounds =  filter(lambda b: not b.name in ["LepFakes"], analysis.backgrounds)
        if ECLF_ARGS.bkg:
            backgrounds = filter(lambda b: b.name in ECLF_ARGS.bkg, backgrounds)

        signals = analysis.get_signals()
        if ECLF_ARGS.sig:
            signals = filter(lambda s: s.name in ECLF_ARGS.sig, signals)
        if ECLF_ARGS.mass_range:
            signals = filter(lambda s:int(ECLF_ARGS.mass_range[0]) <= s.mass <= int(ECLF_ARGS.mass_range[1]), signals)   
    
        dframe = SClassifier.prepare_data(backgrounds, signals, ALL_FEATS[ECLF_ARGS.channel], data_lumi=config.data_lumi,
                                      channel=ECLF_ARGS.channel, branches=BRANCHES[ECLF_ARGS.channel], train_data=ECLF_ARGS.train_data)

        b_df = dframe[dframe["class_label"]==0]
        s_df = dframe[dframe["class_label"]==1]

        if ECLF_ARGS.eval_nn == True:
            s_df["TruthMass"] = s_df.index.get_level_values(0)
            s_df["TruthMass"] = pd.to_numeric(s_df.TruthMass.replace({"Hplus": ""}, regex=True))
            b_df["TruthMass"] = np.random.choice( a=s_df["TruthMass"], size=b_df.shape[0] )

        dframe = pd.concat([b_df, s_df], axis=0)

        jobs = []

        if ECLF_ARGS.eval_nn == True:
            for trained_models, trained_Keras_models in zip(models.values(), Keras_models.values()): 
                plot_scores(trained_models, trained_Keras_models, dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=ECLF_ARGS.outdir, formats=[".pdf", ".png"], inclusive_trks=ECLF_ARGS.inc_trks, isNN=ECLF_ARGS.eval_nn)
        else:
            for mtag, trained_models in models.iteritems():
                jobs += [Job(plot_scores, trained_models,
                   dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=ECLF_ARGS.outdir, formats=[".pdf", ".eps"], inclusive_trks=ECLF_ARGS.inc_trks)]

            if ECLF_ARGS.parallel:
               log.info("************** submitting %i jobs  ************" % len(jobs))
               log.info("***********************************************")
               run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)
            else:
               run_pool(jobs, n_jobs=1)

    if ECLF_ARGS.indirect:
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # # - -  some checks on cmd args
        if ECLF_ARGS.fields:
            fields = filter(lambda v: v.name in ECLF_ARGS.fields, BDT_SCORES[ECLF_ARGS.channel][ECLF_ARGS.bin_scheme])
        else:
            fields = BDT_SCORES[ECLF_ARGS.channel][ECLF_ARGS.bin_scheme]

        if ECLF_ARGS.categories:
            categories = filter(
                lambda c: c.name in ECLF_ARGS.categories, config.categories)
        else:
            categories = config.categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if ECLF_ARGS.systematics:
            systematics = filter(
                lambda s: s.name in ECLF_ARGS.systematics, all_systematics)
        elif ECLF_ARGS.systs:
            systematics = []# all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # - -  if you wish to look at specific samples
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        if ECLF_ARGS.cluster:
            ## setup the submit dir
            os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists".format(ECLF_ARGS.submitdir))
            submitfile_name = "%s/%s.sh" % (ECLF_ARGS.submitdir, "submitAllJobs")
            submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()
            ## pickle the analysis: IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
            with open(ECLF_ARGS.submitdir+ECLF_ARGS.pickled_analysis, "wb") as pfile:
                log.info("Creating the pickled analysis file")
                dill.dump(analysis, pfile)

            if ECLF_ARGS.rs_manager == "CONDOR":
                from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

                ## zip source code
                log.info("Creating source code tarball...")
                source_code_tarball = os.path.abspath(ECLF_ARGS.submitdir+"/source_code.tar.gz")
                if os.path.isfile(source_code_tarball):
                    os.system("rm -rf %s"%source_code_tarball)

                src_ds = ["bin", "hpana", "aux", "setup.sh", ECLF_ARGS.pickled_analysis]
                src_ds = " ".join(src_ds)
                os.system("cd {src_dir} && tar --exclude 'symlinks/*' -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar}".format(
                    src_dir=__HERE+"/../", target_tar=source_code_tarball, source_files=src_ds))


                # wrkDirectory = "/disk/eparrish/databank/"
                # - - setup the submit dir
                os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(ECLF_ARGS.submitdir))

                if ECLF_ARGS.bin_scheme == "SINGLE" or ECLF_ARGS.bin_scheme == "UP_DOWN":
                    from hpana.mva import TRAINING_MASS_BINS
                    indivmodels = {"%sto%s"%(i[0],i[-1]): [] for i in TRAINING_MASS_BINS[ECLF_ARGS.bin_scheme]}
                    for mass_point in indivmodels.keys():
                        for model in ECLF_ARGS.models:
                            if mass_point in os.path.basename(model):
                                indivmodels[mass_point].append(model)
                            else:
                                continue

                    # log.info("************** creating %i jobs for HTCondor submission ************" % len(sum(len(v)for v in indivmodels.values())))
                    # log.info("**************************************************************")

                    for mass_point in indivmodels:

                        submit_file_name = "%s/submitAllJobs%s.sh" % (ECLF_ARGS.outdir, mass_point)
                        submit_file = open(submit_file_name, "w")
                        submit_file.write(
                                         CONDOR_JOB_TEMPLATE.format(
                                             logsdir=os.path.join(
                                                 ECLF_ARGS.submitdir, mass_point, ECLF_ARGS.logsdir),
                                             execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                             userEmail="Z1832314@students.niu.edu",
                                             memory="3GB",
                                         ))

                        os.system("mkdir -p {0}/{1}/jobs  {0}/{1}/logs {0}/{1}/hists {0}/{1}/logs/log {0}/{1}/logs/err {0}/{1}/logs/out".format(ECLF_ARGS.submitdir, mass_point))
                        for aw in ana_workers:

                            submit_file.write("\nArguments = %s %s %s %s %s %s \nqueue\n" %(     aw.name,  
                                                                                            os.path.join(__HERE, "classifier-evaluator"), 
                                                                                            os.path.join(os.getcwd(), ECLF_ARGS.submitdir), 
                                                                                            os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                            ",".join([wrkDirectory+os.path.basename(m) for m in ECLF_ARGS.models]),
                                                                                            ECLF_ARGS.outdir,
                                                                                            )
                        )

                        submit_file.close()
                        log.warning("Submission not supported from within container.")
                        log.warning("Please launch outside container via:")
                        log.warning("\t condor_submit {0}".format(submit_file_name)) 


                else:
                    submit_file_name = "%s/submitAllJobs.sh" % (ECLF_ARGS.submitdir)
                    submit_file = open(submit_file_name, "w")
                    submit_file.write(
                                     CONDOR_JOB_TEMPLATE.format(
                                         logsdir=os.path.join(
                                             ECLF_ARGS.submitdir, ECLF_ARGS.logsdir),
                                         execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                         userEmail="Z1832314@students.niu.edu",
                                         memory="3GB",
                                     ))
                    log.info("************** creating %i jobs for HTCondor submission ************" % len(ECLF_ARGS.models))
                    log.info("**************************************************************")

                    for aw in ana_workers:
                        submit_file.write("\nArguments = %s %s %s %s %s %s \nqueue\n" %(     aw.name,  
                                                                                            os.path.join(__HERE, "classifier-evaluator"), 
                                                                                            os.path.join(os.getcwd(), ECLF_ARGS.submitdir), 
                                                                                            os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                            ",".join([os.path.basename(m) for m in ECLF_ARGS.models]),
                                                                                            ECLF_ARGS.outdir,
                                                                                            )
                        )

                        # - - - - write submission to file to be used outside container
                        log.info("Writing to %s for job %s ..." %
                                 (submit_file_name, aw.name))
                        # os.write(submit_file, "sbatch %s\n" % jfile_name)

                    submit_file.close()
                    log.warning("Submission not supported from within container.")
                    log.warning("Please launch outside container via:")
                    log.warning("\t condor_submit {0}".format(submit_file_name))

    if ECLF_ARGS.direct:
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # # - -  some checks on cmd args
        if ECLF_ARGS.fields:
            fields = filter(lambda v: v.name in ECLF_ARGS.fields, BDT_SCORES[ECLF_ARGS.channel][ECLF_ARGS.bin_scheme])
        else:
            fields = BDT_SCORES[ECLF_ARGS.channel][ECLF_ARGS.bin_scheme]

        if ECLF_ARGS.categories:
            categories = filter(
                lambda c: c.name in ECLF_ARGS.categories, config.categories)
        else:
            categories = config.categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if ECLF_ARGS.systematics:
            systematics = filter(
                lambda s: s.name in ECLF_ARGS.systematics, all_systematics)
        elif ECLF_ARGS.systs:
            systematics = []# all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # - -  if you wish to look at specific samples
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        if ECLF_ARGS.cluster == False:
            models, Keras_models = get_models(ECLF_ARGS.models, isNN=ECLF_ARGS.eval_nn)
            ## prepare hist workers 
            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()

            jobs = []
            for w in ana_workers:
                if ECLF_ARGS.eval_nn == True:
                    dataset_hists_direct(w, clf_models=models, clf_Keras_models=Keras_models, write_hists=True, outdir=ECLF_ARGS.outdir, isNN=ECLF_ARGS.eval_nn)
                else:
                    ## multicore processing 
                    if ECLF_ARGS.parallel:
                        jobs.append(Job(dataset_hists_direct, w, clf_models=models, clf_Keras_models=Keras_models, write_hists=True, outdir=ECLF_ARGS.outdir, isNN=ECLF_ARGS.eval_nn))
                    else:
                        dataset_hists_direct(w, clf_models=models, write_hists=True, outdir=ECLF_ARGS.outdir, isNN=ECLF_ARGS.eval_nn)

            if ECLF_ARGS.parallel:
                log.info("************** submitting %i jobs  ************" % len(jobs))
                log.info("***********************************************")
                run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)

        ## submit to the batch system 
        if ECLF_ARGS.cluster:
            ## retry failed jobs ?
            if ECLF_ARGS.retry:
                # submitted_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/submitted*"%ECLF_ARGS.outdir)])
                submitted_jobs = set([j.split("/")[-1].replace(".slurm", "") for j in glob.glob("%s/jobs/*.slurm"%ECLF_ARGS.submitdir)])
                completed_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/done*"%ECLF_ARGS.submitdir)])
        
                failed_jobs = []
                for j in submitted_jobs:
                    if not j in completed_jobs:
                            if not j in failed_jobs:
                                failed_jobs += [j]

                log.warning("Following %i jobs are failed; you have to resubmit them"%len(failed_jobs))
                submit_file = open("%s/submit_%i_failed.sh"%(ECLF_ARGS.submitdir, len(failed_jobs)), "w")

                for j in failed_jobs:
                    log.info("updating job script for %s"%j) 
                    submit_file.write("sbatch %s/jobs/%s.slurm \n"%(ECLF_ARGS.submitdir, j))
                submit_file.close()
                return 
                
            ## setup the submit dir
            os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists".format(ECLF_ARGS.submitdir))
            submitfile_name = "%s/%s.sh" % (ECLF_ARGS.submitdir, "submitAllJobs")
            submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()
            ## pickle the analysis: IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
            with open(ECLF_ARGS.pickled_analysis, "wb") as pfile:
                log.info("Creating the analysis pickle file")
                dill.dump(analysis, pfile)

            if ECLF_ARGS.rs_manager=="SLURM":
                from hpana.cluster.job_template import SLURM_JOB_TEMPLATE
                for aw in ana_workers:
                    if ECLF_ARGS.dry_run:
                        print "--"*100
                        print aw.name
                        continue
                    # if not ".".join(aw.name.split(".")[:-1]) in TMP_DATASETS:
                    #     continue
                        
                    ## payload for container
                    payload = "source {job_script} {script_path} {pickled_analysis} {models}".format(
                        job_script=os.path.join(__HERE, "slurm_job_evalclf.sh"),
                        script_path=os.path.join(__HERE, "classifier-evaluator"),
                        pickled_analysis=os.path.abspath(ECLF_ARGS.pickled_analysis),
                        models=",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                    )

                    ## write the job submission script with right permissions
                    jfile_name = "%s/jobs/%s.slurm" % (ECLF_ARGS.outdir, aw.name)
                    jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                    os.write(jfile,
                            SLURM_JOB_TEMPLATE.format(
                                logsdir=os.path.join(
                                    ECLF_ARGS.outdir, ECLF_ARGS.logsdir),
                                outdir=ECLF_ARGS.outdir,
                                local_scratch=ECLF_ARGS.local_scratch,
                                cores=1,
                                time="4:0:0",
                                memory="3GB",
                                jobname=aw.name,  #<! important - jobname steers which worker used from pickled ana
                                payload=payload,
                                project=ECLF_ARGS.rs_project
                            )
                            )

                    os.close(jfile)

                    ## write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                            (submitfile_name, aw.name))
                    os.write(submit_file, "sbatch %s\n" % jfile_name)

                os.close(submit_file)
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t sh {0}".format(submitfile_name))       

            elif ECLF_ARGS.rs_manager == "CONDOR":
                from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

                ## zip source code
                log.info("Creating source code tarball...")
                source_code_tarball = os.path.abspath(ECLF_ARGS.submitdir+"/source_code.tar.gz")
                if os.path.isfile(source_code_tarball):
                    os.system("rm -rf %s"%source_code_tarball)

                src_ds = ["bin", "hpana", "aux", "setup.sh", ECLF_ARGS.pickled_analysis]
                src_ds = " ".join(src_ds)
                os.system("cd {src_dir} && tar --exclude 'symlinks/*' -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar}".format(
                    src_dir=__HERE+"/../", target_tar=source_code_tarball, source_files=src_ds))


                # wrkDirectory = "/disk/eparrish/databank/"
                # - - setup the submit dir
                os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(ECLF_ARGS.submitdir))

                # if ECLF_ARGS.bin_scheme == "SINGLE" or "ALL" or ECLF_ARGS.bin_scheme == "UP_DOWN":
                if ECLF_ARGS.bin_scheme == "UP_DOWN":# or ECLF_ARGS.bin_scheme == "SINGLE":
                    from hpana.mva import TRAINING_MASS_BINS
                    indivmodels = {"%sto%s"%(i[0],i[-1]): [] for i in TRAINING_MASS_BINS[ECLF_ARGS.bin_scheme]}
                    for mass_point in indivmodels.keys():
                        for model in ECLF_ARGS.models:
                            if mass_point in os.path.basename(model):
                                indivmodels[mass_point].append(model)
                            else:
                                continue



                    # if ECLF_ARGS.bin_scheme=="UP_DOWN":
                        #     log.info("%s, %s, %s"%(signal_masses[0],signal_masses[-1],signal_masses))
                        #     f_sig = signal_masses[0]
                        #     l_sig = signal_masses[-1]

                        #     signals = filter(lambda s:f_sig <= s.mass <= l_sig, SIGS)
                        # else:
                        #     signals = filter(lambda s: s.mass in signal_masses, SIGS)

                        # log.info("************** creating %i jobs for HTCondor submission ************" % len(sum(len(v)for v in indivmodels.values())))
                        # log.info("**************************************************************")

                        # log.info(indivmodels)

                    submit_file_name = "%s/submitAllJobs.sh" % (ECLF_ARGS.submitdir)
                    submit_file = open(submit_file_name, "w")

                    submit_file.write(
                                         CONDOR_JOB_TEMPLATE.format(
                                             logsdir=os.path.join(
                                                 ECLF_ARGS.submitdir, ECLF_ARGS.logsdir),
                                             execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                             memory="3GB",
                                         ))

                    for thismass in indivmodels.keys():

                        os.system("mkdir -p {0}/{1}/jobs  {0}/{1}/logs {0}/{1}/hists {0}/{1}/logs/log {0}/{1}/logs/err {0}/{1}/logs/out".format(ECLF_ARGS.submitdir, thismass))

                        log.debug("len(ana_workers): ", len(ana_workers))
                        #split_workers = list(split(ana_workers, 100))
                        #print "len(split_workers): ", len(split_workers)
                        split_workers = list(chunks(ana_workers, 50))
                        log.debug("len(split_workers): ", len(split_workers))

                        log.info("************** creating %i jobs for HTCondor submission ************" % len(split_workers))
                        log.info("**************************************************************")

                        # print thismass
                        l_mass = int(thismass.split("to")[0])
                        h_mass = int(thismass.split("to")[1])
                        for aw in split_workers:
                            # print thismass
                            # print len(aw)
                            trimmed_aw = []
                            for i in aw:
                                if "hplus" in i.name.lower():
                                    # print "\t\t%s"  %(i.sample[5:])
                                    worker_mass = int(i.sample[5:])
                                    if worker_mass > l_mass and worker_mass < h_mass:
                                        # print "\t%s" %(i.name)
                                        # print "\t\t%s, %s, %s" %(l_mass, worker_mass, h_mass)
                                        trimmed_aw.append(i)
                                    else:
                                        # print "Removing %s from %s" %(i.name, thismass)
                                        # aw.remove(i)
                                        continue
                                else:
                                    trimmed_aw.append(i)
                            # print "\t%s" %len(aw)
                            # these_models=[]
                            # for worker in aw:
                            #     if mass_point in worker.name:
                            #         these_workers.append(worker)

                            # submit_file.write("\nArguments = %s %s %s %s %s %s %s\nqueue\n" %(  aw.name, 
                            #                                                                     source_code_tarball, 
                            #                                                                     os.path.join(__HERE, "classifier-evaluator"), 
                            #                                                                     os.path.join(os.getcwd(), 
                            #                                                                     ECLF_ARGS.outdir,mass_point), 
                            #                                                                     os.path.abspath(ECLF_ARGS.pickled_analysis), 
                            #                                                                     os.path.abspath(ECLF_ARGS.models_path[0]), 
                            #                                                                     ",".join(os.path.basename(m) for m in indivmodels[mass_point])))

                            if len(trimmed_aw) != 0:
                                submit_file.write("\nArguments = %s %s %s %s %s %s %s \nqueue\n" %(     ",".join([m.name for m in trimmed_aw]), 
                                                                                                os.path.join(__HERE, "classifier-evaluator"), 
                                                                                                os.path.join(os.getcwd(), ECLF_ARGS.submitdir, thismass), 
                                                                                                os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                                ",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                                                                                                # ",".join([os.path.abspath(m) for m in indivmodels[thismass]]),
                                                                                                int(ECLF_ARGS.eval_nn),
                                                                                                ECLF_ARGS.outdir,
                                                                                            )
                        )

                        # submit_file.close()
                        # log.warning("Submission not supported from within container.")
                        # log.warning("Please launch outside container via:")
                        # log.warning("\t condor_submit {0}".format(submit_file_name)) 

                    submit_file.close()
                    log.warning("Submission not supported from within container.")
                    log.warning("Please launch outside container via:")
                    log.warning("\t condor_submit {0}".format(submit_file_name)) 


                else:

                    submit_file_name = "%s/submitAllJobs.sh" % (ECLF_ARGS.submitdir)
                    submit_file = open(submit_file_name, "w")
                    submit_file.write(
                                     CONDOR_JOB_TEMPLATE.format(
                                         logsdir=os.path.join(
                                             ECLF_ARGS.submitdir, ECLF_ARGS.logsdir),
                                         execScript=os.path.join(__HERE, "condor_jobs_clf_eval.sh"),
                                         memory="3GB",
                                     ))

                    
                    log.debug("len(ana_workers): ", len(ana_workers))
                    #split_workers = list(split(ana_workers, 100))
                    #print "len(split_workers): ", len(split_workers)
                    split_workers = list(chunks(ana_workers, 100))
                    log.debug("len(split_workers): ", len(split_workers))

                    log.info("************** creating %i jobs for HTCondor submission ************" % len(split_workers))
                    log.info("**************************************************************")

                    #for aw in split_workers:
                    #   print "printout: ", ",".join([m.name for m in aw])

                    #for aw in ana_workers:
                    for aw in split_workers:

                        # istops = False
                        # diffSamples = False
                        # last_sample = aw[0].name.split(".")[0].lower()
                        # smaller_split_workers=[]
                        # last_index = 0

                        # for i in range(len(aw)):
                        #     # if "ttbar" in aw[i].name.lower() or "singletop" in aw[i].name.lower():
                        #     #     istops = True


                        #     if last_sample != aw[i].name.split(".")[0].lower():
                        #         diffSamples = True
                        #         print last_sample,  aw[i].name.split(".")[0].lower()

                        #         smaller_split_workers+=[aw[last_index:i],aw[i:]]


                        #         last_sample = aw[i].name.split(".")[0].lower()
                        #         last_index = i

                        #     # print i.name.split(".")
                        #     # print i.name.split(".")[0].lower() 

                        # if diffSamples == True:
                        #     print "diffSamples = True!!!"
                        #     # print smaller_split_workers
                        #     # for smaller_workers in smaller_split_workers:
                        #     for wrks in smaller_split_workers:
                        #         submit_file.write("\nArguments = %s %s %s %s %s %d \nqueue\n" %(     ",".join([m.name for m in wrks]),
                        #                                                                         os.path.join(__HERE, "classifier-evaluator"), 
                        #                                                                         os.path.join(os.getcwd(), ECLF_ARGS.outdir), 
                        #                                                                         os.path.abspath(ECLF_ARGS.pickled_analysis), 
                        #                                                                         #",".join([wrkDirectory+os.path.basename(m) for m in ECLF_ARGS.models])
                        #                                                                         ",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                        #                                                                         int(ECLF_ARGS.eval_nn)
                        #                                                                         )
                        #             )
                        #     # smaller_split_workers = []

                        # # if istops == True:
                        # #     smaller_workers = list(chunks(aw,2))
                        # #     print len(smaller_workers)
                        # #     for wrks in smaller_workers:
                        # #         submit_file.write("\nArguments = %s %s %s %s %s %d \nqueue\n" %(     ",".join([m.name for m in wrks]),
                        # #                                                                     os.path.join(__HERE, "classifier-evaluator"), 
                        # #                                                                     os.path.join(os.getcwd(), ECLF_ARGS.outdir), 
                        # #                                                                     os.path.abspath(ECLF_ARGS.pickled_analysis), 
                        # #                                                                     #",".join([wrkDirectory+os.path.basename(m) for m in ECLF_ARGS.models])
                        # #                                                                     ",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                        # #                                                                     int(ECLF_ARGS.eval_nn)
                        # #                                                                     )
                        # #         )

                        # else:

                        submit_file.write("\nArguments = %s %s %s %s %s %d %s \nqueue\n" %(     ",".join([m.name for m in aw]),
                                                                                            os.path.join(__HERE, "classifier-evaluator"), 
                                                                                            os.path.join(os.getcwd(), ECLF_ARGS.submitdir), 
                                                                                            os.path.abspath(ECLF_ARGS.pickled_analysis), 
                                                                                            #",".join([wrkDirectory+os.path.basename(m) for m in ECLF_ARGS.models])
                                                                                            ",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                                                                                            int(ECLF_ARGS.eval_nn),
                                                                                            ECLF_ARGS.outdir,
                                                                                            )
                         )



                        # - - - - write submission to file to be used outside container
                        # log.info("Writing to %s for job %s ..." %
                                 #(submit_file_name, aw.name))
                                 # (submit_file_name, ",".join([m.name for m in aw])))
                        # os.write(submit_file, "sbatch %s\n" % jfile_name)

                    submit_file.close()


                    # for aw in ana_workers:

                    #     # submit_file.write("\nArguments = %s %s %s %s %s %s %s\nqueue\n" %(  aw.name, 
                    #     #                                                                     source_code_tarball, 
                    #     #                                                                     os.path.join(__HERE, "classifier-evaluator"), 
                    #     #                                                                     os.path.join(os.getcwd(), ECLF_ARGS.outdir), 
                    #     #                                                                     os.path.abspath(ECLF_ARGS.pickled_analysis), 
                    #     #                                                                     os.path.abspath(ECLF_ARGS.models_path[0]), 
                    #     #                                                                     ",".join([os.path.basename(m) for m in ECLF_ARGS.models])))


                    #     submit_file.write("\nArguments = %s %s %s %s %s \nqueue\n" %(     aw.name,  
                    #                                                                         os.path.join(__HERE, "classifier-evaluator"), 
                    #                                                                         os.path.join(os.getcwd(), ECLF_ARGS.outdir), 
                    #                                                                         os.path.abspath(ECLF_ARGS.pickled_analysis), 
                    #                                                                         ",".join([wrkDirectory+os.path.basename(m) for m in ECLF_ARGS.models])
                    #                                                                         )
                    #     )



                    #     # - - - - write submission to file to be used outside container
                    #     log.info("Writing to %s for job %s ..." %
                    #              (submit_file_name, aw.name))
                    #     # os.write(submit_file, "sbatch %s\n" % jfile_name)

                    # submit_file.close()
                    log.warning("Submission not supported from within container.")
                    log.warning("Please launch outside container via:")
                    log.warning("\t condor_submit {0}".format(submit_file_name)) 
   

    if ECLF_ARGS.merge_hists:
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)
        
        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # - -  if you wish to look at specific samples
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        # ## - - prepare testing dataFrame 
        # backgrounds =  filter(lambda b: not b.name in ["LepFakes"], analysis.backgrounds)
        # if ECLF_ARGS.bkg:
        #     backgrounds = filter(lambda b: b.name in ECLF_ARGS.bkg, backgrounds)

        # signals = analysis.get_signals()
        # if ECLF_ARGS.sig:
        #     signals = filter(lambda s: s.name in ECLF_ARGS.sig, signals)
        # if ECLF_ARGS.mass_range:
        #     signals = filter(lambda s:int(ECLF_ARGS.mass_range[0]) <= s.mass <= int(ECLF_ARGS.mass_range[1]), signals)   
        log.info(
            "************** merging histograms  ************")
        log.info(
            "***********************************************")
        analysis.merge_hists(samples=samples, histsdir=ECLF_ARGS.outdir, overwrite=True, write=True)


    if ECLF_ARGS.friend:
        ## sort files based on size to start the heavier ones sooner.
        #ECLF_ARGS.files.sort(key=lambda f: os.path.getsize(f), reverse=True)
        # Load keras models after sorting the files
        models, Keras_models = get_models(ECLF_ARGS.models, isNN=ECLF_ARGS.eval_nn)

        jobs = [AppendJob(f, [models, Keras_models], copy_file=COPY_FILE, outdir=ECLF_ARGS.outdir) for f in ECLF_ARGS.files]

        st = time.time()
        ## run a pool of jobs
        if False: #ECLF_ARGS.parallel or len(jobs)> 1: # TODO Keras models need to be re-loaded in child jobs
            run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)
        else:
            ## processing one file only (also for PBS, CONDOR Batch)
            for job in jobs:
                job.run()

    ftime = time.time()
    elapsed_time = (ftime - itime)
    log.info("\n****************** elapsed time: %i sec ******************" % elapsed_time)


if __name__=='__main__':
    main()
