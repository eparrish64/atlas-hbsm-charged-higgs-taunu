#!/usr/bin/env python2.7

## stdl
import sys, os, re, gc, glob, argparse
import cPickle, dill
from os import environ
import numpy as np
import pandas as pd

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_evalclf_parser 
eclf_parser = get_evalclf_parser()


ECLF_ARGS = eclf_parser.parse_args()


## local
from hpana.variables import CLF_FEATURES, BDT_SCORES
from hpana.dataset_hists import dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration
from hpana.mva.classifier import Classifier, SClassifier
from hpana.mva import N_TRACKS, PKL_FILE_PATTERN, XML_FILE_PATTERN, ALL_FEATS, BRANCHES 
from hpana.mva.evaluation import AppendJob, plot_scores, get_models, evaluate_scores_on_trees

#ak
#environ['KERAS_BACKEND'] = 'theano'
environ['KERAS_BACKEND'] = 'tensorflow'
# Set architecture of system (AVX instruction set is not supported on SWAN)
environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, BatchNormalization
from keras.regularizers import l2
from keras import initializers
from keras.optimizers import SGD
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import load_model
#ak

## setup logger
from hpana import log
log.setLevel(ECLF_ARGS.log)
 
## Setup ROOT
import ROOT
from ROOT import TMVA
ROOT.gROOT.SetBatch(True)


#-----------------------------------------------
# consts
#-----------------------------------------------
TMP_DATASETS = [ 
    "LepFakes.Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "LepFakes.Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV280_500_CVetoBVeto_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV1000_E_CMS_9364",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV140_280_BFilter_9364__001",
    "Wtaunu.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV500_1000_9364",
    "Ztautau.Sherpa_221_NNPDF30NNLO_Ztautau_MAXHTPTV500_1000_9364",
]

__HERE = os.path.dirname(os.path.abspath(__file__))

COPY_FILE = True
USE_FASTBDT = False
kFOLDS = ECLF_ARGS.kfolds
METHOD_TYPE = TMVA.Types.kBDT #<! TMVA method 

if ECLF_ARGS.backend=="tmva":
    ## initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
if ECLF_ARGS.mtype=="keras":
    METHOD_TYPE = TMVA.Types.kPyKeras
    ## - -  Select Theano as backend for Keras
    os.environ['KERAS_BACKEND'] = 'theano'
    
    ## - - Set architecture of system (AVX instruction set is not supported on SWAN)
    environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'


##-----------------------------------------------
## main driver
##-----------------------------------------------
def main():
    import time
    itime = time.time()
    if not ECLF_ARGS.models:
        raise IOError('Path to trained models pls ?')        

    ## - - setup outdir
    os.system("mkdir -p %s"%ECLF_ARGS.outdir)

    if ECLF_ARGS.plot_scores:
        #models = get_models(ECLF_ARGS.models)
        models, Keras_models = get_models(ECLF_ARGS.models)
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)
        
        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        ## - - prepare testing dataFrame 
        backgrounds =  filter(lambda b: not b.name in ["LepFakes"], analysis.backgrounds)
        if ECLF_ARGS.bkg:
            backgrounds = filter(lambda b: b.name in ECLF_ARGS.bkg, backgrounds)

        signals = analysis.get_signals()
        if ECLF_ARGS.sig:
            signals = filter(lambda s: s.name in ECLF_ARGS.sig, signals)
        if ECLF_ARGS.mass_range:
            signals = filter(lambda s:int(ECLF_ARGS.mass_range[0]) <= s.mass <= int(ECLF_ARGS.mass_range[1]), signals)   
    
        dframe = SClassifier.prepare_data(backgrounds, signals, ALL_FEATS[ECLF_ARGS.channel], data_lumi=config.data_lumi,
                                      channel=ECLF_ARGS.channel, branches=BRANCHES[ECLF_ARGS.channel], train_data=ECLF_ARGS.train_data)

        b_df = dframe[dframe["class_label"]==0]
        s_df = dframe[dframe["class_label"]==1]
        s_df["TruthMass"] = s_df.index.get_level_values(0)
        s_df["TruthMass"] = pd.to_numeric(s_df.TruthMass.replace({"Hplus": ""}, regex=True))
        b_df["TruthMass"] = np.random.choice( a=s_df["TruthMass"], size=b_df.shape[0] )
        dframe = pd.concat([b_df, s_df], axis=0)

        jobs = []
        #for mtag, trained_models in models.iteritems():
        for trained_models, trained_Keras_models in zip(models.values(), Keras_models.values()): 
            plot_scores(trained_models, trained_Keras_models, dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=ECLF_ARGS.outdir, formats=[".pdf", ".png"], inclusive_trks=ECLF_ARGS.inc_trks)
            #jobs += [Job(plot_scores, trained_models, trained_Keras_models, 
            #    dframe=dframe, backgrounds=backgrounds, signals=signals, outdir=ECLF_ARGS.outdir, formats=[".pdf", ".png"], inclusive_trks=ECLF_ARGS.inc_trks)]

        #if ECLF_ARGS.parallel:
        #    log.info("************** submitting %i jobs  ************" % len(jobs))
        #    log.info("***********************************************")
        #    run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)
        #else:
        #    run_pool(jobs, n_jobs=1)

    if ECLF_ARGS.direct:
        # - -  build analysis main configuration object
        config = Configuration(ECLF_ARGS.channel, data_streams=ECLF_ARGS.data_streams, db_version=ECLF_ARGS.db_version)

        # - -  instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # # - -  some checks on cmd args
        if ECLF_ARGS.fields:
            fields = filter(lambda v: v.name in ECLF_ARGS.fields, BDT_SCORES[ECLF_ARGS.channel])
        else:
            fields = BDT_SCORES[ECLF_ARGS.channel]

        if ECLF_ARGS.categories:
            categories = filter(
                lambda c: c.name in ECLF_ARGS.categories, config.categories)
        else:
            categories = config.categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if ECLF_ARGS.systematics:
            systematics = filter(
                lambda s: s.name in ECLF_ARGS.systematics, all_systematics)
        elif ECLF_ARGS.systs:
            systematics = []# all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # - -  if you wish to look at specific samples
        if ECLF_ARGS.samples:
            samples = filter(lambda s: s.name in ECLF_ARGS.samples, analysis.samples)
        else:
            samples = analysis.samples #get_signals()

        ## multicore processing 
        if ECLF_ARGS.parallel:
            #models = get_models(ECLF_ARGS.models)
            models, Keras_models = get_models(ECLF_ARGS.models)
            ## prepare hist workers 
            #print "fields: ", fields
            #print "fields.name: ", fields.name
            #for field in fields:
            #    print "field", field

            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()

            jobs = []
            for w in ana_workers:
                jobs.append(Job(dataset_hists_direct, w, clf_models=models, clf_Keras_models=Keras_models, write_hists=True, outdir=ECLF_ARGS.outdir))
                dataset_hists_direct (w, clf_models=models, clf_Keras_models=Keras_models, write_hists=True, outdir=ECLF_ARGS.outdir)

            log.info("************** submitting %i jobs  ************" % len(jobs))
            log.info("***********************************************")
            #run_pool(jobs, n_jobs=ECLF_ARGS.ncpu)

        ## submit to the batch system 
        if ECLF_ARGS.cluster:
            ## retry failed jobs ?
            if ECLF_ARGS.retry:
                # submitted_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/submitted*"%ECLF_ARGS.outdir)])
                submitted_jobs = set([j.split("/")[-1].replace(".slurm", "") for j in glob.glob("%s/jobs/*.slurm"%ECLF_ARGS.outdir)])
                completed_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/done*"%ECLF_ARGS.outdir)])
        
                failed_jobs = []
                for j in submitted_jobs:
                    if not j in completed_jobs:
                            if not j in failed_jobs:
                                failed_jobs += [j]

                log.warning("Following %i jobs are failed; you have to resubmit them"%len(failed_jobs))
                submit_file = open("%s/submit_%i_failed.sh"%(ECLF_ARGS.outdir, len(failed_jobs)), "w")

                for j in failed_jobs:
                    log.info("updating job script for %s"%j) 
                    submit_file.write("sbatch %s/jobs/%s.slurm \n"%(ECLF_ARGS.outdir, j))
                submit_file.close()
                return 
                
            ## setup the submit dir
            os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists".format(ECLF_ARGS.outdir))
            submitfile_name = "%s/%s.sh" % (ECLF_ARGS.outdir, "submitAllJobs")
            submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
            ana_workers = analysis.getWorkers()
            ## pickle the analysis: IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
            with open(ECLF_ARGS.pickled_analysis, "wb") as pfile:
                dill.dump(analysis, pfile)

            if ECLF_ARGS.rs_manager=="SLURM":
                from hpana.cluster.job_template import SLURM_JOB_TEMPLATE

                for aw in ana_workers:
                    if ECLF_ARGS.dry_run:
                        print "--"*100
                        print aw.name
                        continue
                    # if not ".".join(aw.name.split(".")[:-1]) in TMP_DATASETS:
                    #     continue
                        
                    ## payload for container
                    payload = "source {job_script} {script_path} {pickled_analysis} {models}".format(
                        job_script=os.path.join(__HERE, "slurm_job_evalclf.sh"),
                        script_path=os.path.join(__HERE, "classifier-evaluator"),
                        pickled_analysis=os.path.abspath(ECLF_ARGS.pickled_analysis),
                        models=",".join([os.path.abspath(m) for m in ECLF_ARGS.models]),
                    )

                    ## write the job submission script with right permissions
                    jfile_name = "%s/jobs/%s.slurm" % (ECLF_ARGS.outdir, aw.name)
                    jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                    os.write(jfile,
                            SLURM_JOB_TEMPLATE.format(
                                logsdir=os.path.join(
                                    ECLF_ARGS.outdir, ECLF_ARGS.logsdir),
                                outdir=ECLF_ARGS.outdir,
                                local_scratch=ECLF_ARGS.local_scratch,
                                cores=1,
                                time="4:0:0",
                                memory="3GB",
                                jobname=aw.name,  #<! important - jobname steers which worker used from pickled ana
                                payload=payload,
                                project=ECLF_ARGS.rs_project
                            )
                            )

                    os.close(jfile)

                    ## write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                            (submitfile_name, aw.name))
                    os.write(submit_file, "sbatch %s\n" % jfile_name)

                os.close(submit_file)
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t sh {0}".format(submitfile_name))        

        if ECLF_ARGS.merge_hists:
            log.info(
                "************** merging histograms  ************")
            log.info(
                "***********************************************")
            analysis.merge_hists(samples=samples, histsdir=ECLF_ARGS.outdir, overwrite=True, write=True)


    if ECLF_ARGS.append:
        ## sort files based on size to start the heavier ones sooner.
        ECLF_ARGS.files.sort(key=lambda f: os.path.getsize(f), reverse=True)
        jobs = [AppendJob(f, models, copy_file=COPY_FILE, outdir=ECLF_ARGS.outdir) for f in ECLF_ARGS.files]

        st = time.time()
        ## run a pool of jobs
        if ECLF_ARGS.parallel or len(jobs)> 1:
            run_pool(jobs, n_jobs=-1)
        else:
            ## processing one file only (also for PBS, CONDOR Batch)
            for job in jobs:
                job.run()

    ftime = time.time()
    elapsed_time = (ftime - itime)
    log.info("\n****************** elapsed time: %i sec ******************" % elapsed_time)


if __name__=='__main__':
    main()