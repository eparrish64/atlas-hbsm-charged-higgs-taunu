#! /usr/bin/env python
"""
simple script for submitting jobs to a cluster.
"""
 
if __name__=="__main__":
        import sys, os
        import socket
        import time
        import ROOT
        import yaml
        from hpana.dataset_hists import dataset_hists
        from hpana.config import Configuration
        from hpana.analysis import Analysis

        if 'cedar' in socket.gethostname():
            jobname = os.getenv("SLURM_JOB_NAME")
        elif 'niu' in socket.gethostname() or 'cern' in socket.gethostname():
            jobname = sys.argv[2]
        else:
            jobname = os.getenv("PBS_JOBNAME")

        with open(sys.argv[1], "rb") as yfile:
            conf = yaml.load(yfile)

        print jobname

        print "Configuration args:", conf

        # - - - - build analysis main configuration object
        config = Configuration(
            conf["channel"],
            year=conf["year"],
            data_streams=conf["data_streams"],
            mc_campaign=conf["mc_campaign"],
            db_version=conf["db_version"],
            FFs_macros=conf["FFs_macros"],
            metTrigEff_macros=conf["metTrigEff_macros"],
            upsilon_macros=conf["upsilon_macros"]
            )

        # - - - - instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # - - - - some checks on cmd args
        if conf["fields"]:
            fields = filter(lambda v: v.name in conf["fields"], config.variables)
        else:
            fields = config.variables

        if conf["categories"]:
            categories = filter(
                lambda c: c.name in conf["categories"], config.categories+ config.ff_cr_regions+ config.clf_regions)
            # print categories
        else:
            categories = config.categories 
            # print categories

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if conf["systematics"]:
            systematics = filter(
                lambda s: s.name in conf["systematics"], all_systematics)
        elif conf["systs"]:
            systematics = [] #all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        samples = filter(lambda s: s.name in jobname.split(".")[:1], analysis.samples)  #<! DON"T MESS AROUND WITH JOB NAME
        samps = []
        # print jobname.split(",")
        for jo in jobname.split(","):
            # print jo.split(".")[:1]
            samps.append(filter(lambda s: s.name in jo.split(".")[:1], analysis.samples)[0])
        # print samps
        # for samp in samps:
        #     print "Samples: %s" %samp.name

        ## get the worker
        analysis.setWorkers(samples=samps, fields=fields, categories=categories, systematics=systematics)

        ana_workers = analysis.getWorkers()

        # for w in ana_workers:
        #     print w.name 
        
        start_time = time.time()

        # print jobname.split(",")

        for wrk in jobname.split(","):
        # print jobname.split(",")

            workers = filter(lambda w: w.name==wrk, ana_workers)

            # for work in workers:
            #     print "WORKER:\t %s" %(work.name)
            
            if len(workers) < 1:
                raise RuntimeError("no worker with name %s is assigned for the job"%wrk)

            # if len(workers) > 1:
            #     raise RuntimeError("%i workers with the same name %s are assigned the job"%(len(workers), wrk))        

            # worker = workers[0]
            # print "WORKER: ", worker.name
            # for worker in workers:
            # - - get the hists 
            dataset_hists(workers[0], write_hists=True, outdir="./")
        end_time = time.time()
        print "Execution Time:\t %0.2f"%(end_time - start_time)
