#! /usr/bin/env python
"""
simple script for submitting jobs to a cluster.
"""
 
if __name__=="__main__":
        import sys, os
        import socket
        import time
        import ROOT
        import yaml
        from hpana.dataset_hists import dataset_hists
        from hpana.config import Configuration
        from hpana.analysis import Analysis
        from hpana import log 

        jobname = sys.argv[2]

        with open(sys.argv[1], "rb") as yfile:
            conf = yaml.load(yfile)

        print "Configuration args:", conf

        # - - - - build analysis main configuration object
        config = Configuration(
            conf["channel"],
            year=conf["year"],
            data_streams=conf["data_streams"],
            mc_campaign=conf["mc_campaign"],
            db_version=conf["db_version"],
            FFs_macros=conf["FFs_macros"],
            metTrigEff_macros=conf["metTrigEff_macros"],
            upsilon_macros=conf["upsilon_macros"]
            )

        # - - - - instantiate the analysis
        analysis = Analysis(config, compile_cxx=True)

        # - - - - some checks on cmd args
        if conf["fields"]:
            fields = filter(lambda v: v.name in conf["fields"], config.variables)
        else:
            fields = config.variables

        if conf["categories"]:
            categories = filter(
                lambda c: c.name in conf["categories"], config.categories_func(partial_unblind=int(sys.argv[4]))+ config.ff_cr_regions+ config.clf_regions)
        else:
            categories = config.categories_func(partial_unblind=int(sys.argv[4])) 

        ## systematics
        all_systematics = config.systematics[:]  # <! common systematics
        all_systematics += analysis.qcd.systematics  # <! QCD fakes only
        if conf["systematics"]:
            systematics = filter(
                lambda s: s.name in conf["systematics"], all_systematics)
        elif conf["systs"]:
            systematics = [] #all_systematics
        else:
            systematics = config.systematics[:1] #<! NOMINAL

        # samples = filter(lambda s: s.name in jobname.split(".")[:1], analysis.samples)  #<! DON"T MESS AROUND WITH JOB NAME

        ## This allows multiple samples to be run over
        samplenames = []
        for job in jobname.split(","):
            samplenames.append(job.split(".")[0])

        samples = filter(lambda s: s.name in samplenames, analysis.samples)

        ## get the worker
        analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
        ana_workers = analysis.getWorkers()
        
        start_time = time.time()

        for wrk in jobname.split(","):

            workers = filter(lambda w: w.name==wrk, ana_workers)
            
            if len(workers) < 1:
                # raise RuntimeError("no worker with name %s is assigned for the job"%wrk)
                log.warning("no worker with name %s is assigned for the job"%wrk)
                continue

            if len(workers) > 1:
                # raise RuntimeError("%i workers with the same name %s are assigned the job"%(len(workers), wrk))        
                log.warning("%i workers with the same name %s are assigned the job"%(len(workers), wrk))

            worker = workers[0]

            # - - get the hists 
            try:
                dataset_hists(worker, write_hists=True, outdir="./", frienddir=sys.argv[3])
            except:
                continue

        end_time = time.time()
        print "Execution Time:\t %0.2f"%(end_time - start_time)
