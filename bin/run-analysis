#! /usr/bin/env python

# stdlib
import os
import sys
import time
import glob
import yaml

# - - - - - - - -  parse ana args (needed before ROOT)
from hpana.cmd import get_ana_parser
ana_parser = get_ana_parser()
ANA_ARGS = ana_parser.parse_args()
if not ANA_ARGS.submitdir:
    ANA_ARGS.submitdir = ANA_ARGS.outdir

# local
from hpana import log
from hpana.dataset_hists import dataset_hists, dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration
from hpana.plotting.draw import draw 

# - - - - - - - -  set log level
log.setLevel(ANA_ARGS.log)

# - - - - - - - - Speed things up a bit
import ROOT
ROOT.gROOT.SetBatch(True)
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetStyle("ATLAS")
log.debug("ROOT is in batch mode")

def split(a, n):
    k, m = divmod(len(a), n)
    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))

def chunks(a, n):
    for i in xrange(0, len(a), n):
        yield a[i:i+n]

###
## Setting up friend file directory correctly
ANA_ARGS.frienddir += ANA_ARGS.channel + "/"
###

# -------------------------------------------------------------
# consts
# -------------------------------------------------------------
__HERE = os.path.dirname(os.path.abspath(__file__))

# - - - - build analysis main configuration object
config = Configuration(
    ANA_ARGS.channel,
    year=ANA_ARGS.year,
    data_streams=ANA_ARGS.data_streams,
    mc_campaign=ANA_ARGS.mc_campaign,
    db_version=ANA_ARGS.db_version,
    FFs_macros=["FFsCOM.cxx", "FFsCR.cxx"], 
    metTrigEff_macros=["metTrigEff.cxx"], 
    upsilon_macros=["CorrectUpsilon.cxx"],
    effm_taujet_macros=["eff_mass_taujet.cxx"],
    effm_taulep_macros=["eff_mass_taulep.cxx"],
    njets_macros=["njets.cxx"],
    )

# - - - - instantiate the analysis
analysis = Analysis(config, compile_cxx=not ANA_ARGS.no_cxx, )

# - - - - some checks on cmd args
if ANA_ARGS.fields:
    fields = filter(lambda v: v.name in ANA_ARGS.fields, config.variables)
else:
    fields = config.variables 

if ANA_ARGS.categories:
    categories = filter(
        lambda c: c.name in ANA_ARGS.categories, 
            config.categories_func(partial_unblind=ANA_ARGS.partial_unblind)+ config.ff_cr_regions+ config.clf_regions)#+config.met_trigeff_regions)
else:
    categories = config.categories_func(partial_unblind=ANA_ARGS.partial_unblind) 

## systematics
all_systematics = config.systematics[:]  # <! common systematics
all_systematics += analysis.qcd.systematics  # <! QCD fakes only
all_systematics += analysis.ttbar.systematics  # <! TTBar reweighting only
if ANA_ARGS.systematics:
    systematics = filter(
        lambda s: s.name in ANA_ARGS.systematics, all_systematics)
elif ANA_ARGS.systs:
    systematics = [] #all_systematics
else:
    systematics = config.systematics[:1] #<! NOMINAL

# - - - - if you wish to look at specific samples
if ANA_ARGS.samples:
    samples = filter(lambda s: s.name in ANA_ARGS.samples, analysis.samples)
else:
    samples = analysis.samples

# -------------------------------------------------------------
# main driver
# -------------------------------------------------------------
def main():
    # - - assign workers 
    analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
    ana_workers = analysis.getWorkers()

    # - -  run on multicores
    if ANA_ARGS.dry_run:
        log.info("************** submitting %i jobs  ************" % len(ana_workers))
        for w in ana_workers:
            print "--"*70
            print w

    # - - run on multicores 
    elif ANA_ARGS.parallel:
        jobs = []
        for worker in ana_workers:
            jobs.append(Job(dataset_hists, worker, write_hists=True, outdir=ANA_ARGS.outdir, frienddir=ANA_ARGS.frienddir))

        ostr = "**"*20 + "... Submitting %i jobs ..."% len(jobs) + "**"*20 
        log.info(ostr)
        log.info("*"*len(ostr))
        run_pool(jobs, n_jobs=ANA_ARGS.ncpu)

    # - -  run on cluster
    elif ANA_ARGS.cluster:
        ## retry failed jobs ?
        if ANA_ARGS.retry:
            submitted_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/submitted*"%ANA_ARGS.outdir)])
            completed_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/done*"%ANA_ARGS.outdir)])
    
            failed_jobs = []
            for j in submitted_jobs:
                if not j in completed_jobs:
                        if not j in failed_jobs:
                            failed_jobs += [j]

            log.warning("Following %i jobs are failed; you have to resubmit them"%len(failed_jobs))
            submit_file = open("%s/submit_%i_failed.sh"%(ANA_ARGS.submitdir, len(failed_jobs)), "w")

            for j in failed_jobs:
                log.info("updating job script for %s"%j) 
                submit_file.write("sbatch %s/jobs/%s.slurm \n"%(ANA_ARGS.submitdir, j))
            submit_file.close()

            return 

        ## setup submission dir
        os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(ANA_ARGS.submitdir))
        if not os.path.isdir(ANA_ARGS.outdir):
            os.system("mkdir -p {0}".format(ANA_ARGS.outdir))

        ## Analysis Configuration parameters to be copied to the worker nodes
        conf_params = {
            "channel": ANA_ARGS.channel,
            "year": ANA_ARGS.year,
            "data_streams": ANA_ARGS.data_streams,
            "mc_campaign": ANA_ARGS.mc_campaign,
            "db_version": ANA_ARGS.db_version,
            "FFs_macros": ["FFsCOM.cxx", "FFsCR.cxx"],  
            "metTrigEff_macros": ["metTrigEff.cxx"], 
            "upsilon_macros": ["CorrectUpsilon.cxx",],
            "samples": ANA_ARGS.samples,
            "systematics":ANA_ARGS.systematics,
            "systs":ANA_ARGS.systs,
            # "categories":ANA_ARGS.categories,
            "categories":categories,
            "fields":ANA_ARGS.fields,
        }

        with open(ANA_ARGS.conf_file, "wb") as cfile:
            yaml.dump(conf_params, cfile) 

        ## zip source code
        log.info("Creating source code tarball...")
        source_code_tarball = os.path.abspath(ANA_ARGS.submitdir+"/source_code.tar.gz")
        if os.path.isfile(source_code_tarball):
            os.system("rm -rf %s"%source_code_tarball)

        src_ds = ["bin", "hpana", "aux", "setup.sh"]
        src_ds = " ".join(src_ds)
        os.system("cd {src_dir} && tar --exclude 'symlinks/*' -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar} {conf_file}".format(
            src_dir=__HERE+"/../", conf_file=ANA_ARGS.conf_file, target_tar=source_code_tarball, source_files=src_ds))


        if ANA_ARGS.rs_manager == "CONDOR":
            # -- write jobs
            from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

            submitfile_name = "%s/%s.sh" %(ANA_ARGS.submitdir, "submitAllJobs")
            submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            log.info("************** creating %i jobs for HTCondor submission ************" % len(ana_workers))
            log.info("**************************************************************")

            submit_file_name = "%s/submitAllJobs.sh" % (ANA_ARGS.submitdir)
            submit_file = open(submit_file_name, "w")
            submit_file.write(
                         CONDOR_JOB_TEMPLATE.format(
                             logsdir=os.path.join(
                                 ANA_ARGS.submitdir, ANA_ARGS.logsdir),
                             execScript=os.path.join(__HERE, "condor_jobs.sh"),
                             memory="1GB",
                         )
                         )
            # split_workers = list(chunks(ana_workers, 50))

            # for workers in split_workers:
            #     submit_file.write("\nArguments = %s %s %s %s %s \nqueue\n" %(  ",".join([m.name for m in workers]),
            #                                                                 source_code_tarball, 
            #                                                                 os.path.join(__HERE, "process-dataset"), 
            #                                                                 ANA_ARGS.conf_file,
            #                                                                 os.path.join(os.getcwd(), ANA_ARGS.outdir))
            #                                                         )

            for worker in ana_workers: 
                submit_file.write("\nArguments = %s %s %s %s %s %s %s \nqueue\n" %(  worker.name,
                                                                            source_code_tarball, 
                                                                            os.path.join(__HERE, "process-dataset"), 
                                                                            ANA_ARGS.conf_file,
                                                                            os.path.join(os.getcwd(), ANA_ARGS.outdir),
                                                                            ANA_ARGS.frienddir,
                                                                            int(ANA_ARGS.partial_unblind),)
                                                                    )


            submit_file.close()
            log.warning("Submission not supported from within container.")
            log.warning("Please launch outside container via:")
            log.warning("\t condor_submit {0}".format(submit_file_name))

    

    # - - @NOTE: if you submitted jobs to the cluster wait till they're all done!
    if ANA_ARGS.merge_hists:
        if ANA_ARGS.cluster:
            # raise RuntimeError(
            #     "jobs are submitted to the cluster wait for them to get done")

            if ANA_ARGS.rs_manager == "CONDOR":
                # -- write jobs
                from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

                submitfile_name = "%s/%s.sh" %(ANA_ARGS.submitdir, "submitAllMerge")
                submit_file = os.open(submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

                log.info("************** creating 1 merge jobs for HTCondor submission ************" )
                log.info("**************************************************************")

                submit_file_name = "%s/submitAllMerge.sh" % (ANA_ARGS.submitdir)
                submit_file = open(submit_file_name, "w")
                submit_file.write(
                             CONDOR_JOB_TEMPLATE.format(
                                 logsdir=os.path.join(
                                     ANA_ARGS.submitdir, ANA_ARGS.logsdir),
                                 execScript=os.path.join(__HERE, "condor_merge.sh"),
                                 memory="64GB",
                             )
                             )
                submit_file.write("\nArguments = %s %s %s %s %s \nqueue\n" %(  "Merging",
                                                                                source_code_tarball, 
                                                                                os.path.join(__HERE, "merge-datasets"), 
                                                                                ANA_ARGS.conf_file,
                                                                                ANA_ARGS.outdir)
                                                                        )


                submit_file.close()
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t condor_submit {0}".format(submit_file_name))

        else:
            log.info(
                "************** merging histograms  ************")
            log.info(
                "***********************************************")
            analysis.merge_hists(samples=samples, histsdir=ANA_ARGS.outdir, overwrite=True, write=True,)


# -------------------------------------------------------------
# draw plots
# -------------------------------------------------------------
def draw_plots(hists_file, fields, categories):
    ## draw parameters 
    params = {
        "hists_file":hists_file,
        "backgrounds":analysis.backgrounds,
        "data":analysis.data,
        # "blind": False,
        "signals":analysis.get_signals(masses=[80, 160, 400, 3000]),        
        "signal_scale":1,
        "systematics":ANA_ARGS.systs,
        "output_dir":ANA_ARGS.outdir+"/plots/",
        "logy":ANA_ARGS.logy,
        "logx":False,
        "show_ratio":True,
        "show_pvalue":False,
        "error_bars":True,
        "bin_optimization":False,
        "scale_sig_to_bkg_sum":True,
        "output_formats":ANA_ARGS.fmt,
        # "ratio_range": (0.5, 1.5),
        # "blind_range":(0.5, 1),
    }

    ## setup the output directory for the plots
    if not os.path.isdir(params["output_dir"]):
        os.system("mkdir -p %s"%params["output_dir"])

    jobs = []
    ## draw plots
    for var in fields:
        jparams = dict(params) #<! defensive copy 

        ##@FIXME check if it's a classification score
        m_range = var.name.split("_")[-1].split("to")
        if len(m_range) > 1:
            masses = [int(m) for m in m_range]
            signals = analysis.get_signals(masses=masses)
            jparams["signals"] = signals

        for cat in categories:
            ## signals are very tiny in control regions --> drop them from plots
            if not ("SR_" in cat.name or "CLF" in cat.name):
                jparams["signals"] = []
                jparams["blind_range"] = ()

            jobs += [Job(draw, var, cat, **jparams)]
    run_pool(jobs, n_jobs=-1)

# -------------------------------------------------------------
# run
# -------------------------------------------------------------
if __name__ == "__main__":
    start_time = time.time()

    ## process histograms 
    log.info( 30*"*" + "...Processing histograms..."+30*"*")
    log.info("Samples: %r"%[s.name for s in samples])

    log.info("--"*50)
    log.info("Systematics: %r"%[s.name for s in systematics])

    log.info("--"*50)
    log.info("Categories: %r"%[c.name for c in categories])

    log.info("--"*50)
    log.info("Variables: %r"%[f.name for f in fields])

    main()

    ## draw plots 
    if ANA_ARGS.plots:
        log.info(20*"*" + "...Processing plots..."+ 20*"*")
        if not ANA_ARGS.hists_file:
            if len(glob.glob("%s/HIST*"%ANA_ARGS.outdir)) > 1:
                raise NameError("There are multiple histogram files in %s. Please specify a file with --hists-file option." %(ANA_ARGS.outdir))
            elif len(glob.glob("%s/HIST*"%ANA_ARGS.outdir)) == 0:
                raise NameError("There are no histogram files in %s. Please run with --merge-hists option." %(ANA_ARGS.outdir))
            else:
                hists_file = glob.glob("%s/HIST*"%ANA_ARGS.outdir)[0]    
        else:
            hists_file = ANA_ARGS.hists_file    
        draw_plots(hists_file, fields, categories)

    end_time = time.time()
    elapsed_time = (end_time - start_time)/60.
    log.info(
        "\n****************** elapsed time: %0.1f mins ******************" % elapsed_time)
