#! /usr/bin/env python

# stdlib
import os
import sys
import time
import glob
import pickle

# - - - - - - - -  parse ana args (needed before ROOT)
from hpana.cmd import get_ana_parser
ana_parser = get_ana_parser()
ANA_ARGS = ana_parser.parse_args()


# local
from hpana import log
from hpana.dataset_hists import dataset_hists, dataset_hists_direct
from hpana.cluster.parallel import run_pool, Job
from hpana.analysis import Analysis
from hpana.config import Configuration

# PyPI
import dill


# - - - - - - - -  set log level
log.setLevel(ANA_ARGS.log)

# - - - - - - - - Speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")

# -------------------------------------------------------------
# consts
# -------------------------------------------------------------
__HERE = os.path.dirname(os.path.abspath(__file__))

# - - - - build analysis main configuration object
config = Configuration(
    ANA_ARGS.channel,
    year=ANA_ARGS.year,
    data_streams=ANA_ARGS.data_streams,
    mc_campaign=ANA_ARGS.mc_campaign,
    db_version=ANA_ARGS.db_version)


# - - - - instantiate the analysis
analysis = Analysis(config, compile_cxx=not ANA_ARGS.no_cxx)

# - - - - some checks on cmd args
if ANA_ARGS.fields:
    fields = filter(lambda v: v.name in ANA_ARGS.fields, config.variables)
else:
    fields = config.variables

if ANA_ARGS.categories:
    categories = filter(
        lambda c: c.name in ANA_ARGS.categories, config.categories)
else:
    categories = config.categories

## systematics
all_systematics = config.systematics[:]  # <! common systematics
all_systematics += analysis.qcd.systematics  # <! QCD fakes only
if ANA_ARGS.systematics:
    systematics = filter(
        lambda s: s.name in ANA_ARGS.systematics, all_systematics)
elif ANA_ARGS.systs:
    systematics = []# all_systematics
else:
    systematics = config.systematics[:1] #<! NOMINAL

# - - - - if you wish to look at specific samples
if ANA_ARGS.samples:
    samples = filter(lambda s: s.name in ANA_ARGS.samples, analysis.samples)
else:
    samples = analysis.samples


# -------------------------------------------------------------
# main driver
# -------------------------------------------------------------
def main():
    # - - assign workers 
    analysis.setWorkers(samples=samples, fields=fields, categories=categories, systematics=systematics)
    ana_workers = analysis.getWorkers()

    # - -  run on multicores
    if ANA_ARGS.dry_run:
        log.info("************** submitting %i jobs  ************" % len(ana_workers))
        for w in ana_workers:
            print "--"*70
            print w

    # - - run on multicores 
    elif ANA_ARGS.parallel:
        jobs = []
        for worker in ana_workers:
            jobs.append(Job(dataset_hists, worker, write_hists=True, outdir=ANA_ARGS.outdir))

        log.info("************** submitting %i jobs  ************" % len(jobs))
        log.info("***********************************************")
        run_pool(jobs, n_jobs=ANA_ARGS.ncpu)

    # - -  run on cluster
    elif ANA_ARGS.cluster:
        ## retry failed jobs ?
        if ANA_ARGS.retry:
            submitted_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/submitted*"%ANA_ARGS.outdir)])
            completed_jobs = set([j.split("/")[-1].split("-")[-1] for j in glob.glob("%s/jobs/done*"%ANA_ARGS.outdir)])
    
            failed_jobs = []
            for j in submitted_jobs:
                if not j in completed_jobs:
                        if not j in failed_jobs:
                            failed_jobs += [j]

            log.warning("Following %i jobs are failed; you have to resubmit them"%len(failed_jobs))
            submit_file = open("%s/submit_%i_failed.sh"%(ANA_ARGS.outdir, len(failed_jobs)), "w")

            for j in failed_jobs:
                log.info("updating job script for %s"%j) 
                submit_file.write("sbatch %s/jobs/%s.slurm \n"%(ANA_ARGS.outdir, j))
            submit_file.close()

            return 

        # - -  pickle the analysis: IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
        with open(ANA_ARGS.pickled_analysis, "wb") as pfile:
            dill.dump(analysis, pfile)

        # - - clusters' resource manager and scheduler
        if ANA_ARGS.rs_manager == "TORQUE":
            # - - write jobs
            from hpana.cluster.job_template import PBS_JOB_TEMPLATE

            # - - setup the submit dir
            os.system(
                "mkdir -p {0}/jobs {0}/hists".format(ANA_ARGS.outdir))
            log.info(
                "************** submitting %i jobs to the cluster ************" % len(analysis.workers()))
            log.info(
                "**************************************************************")

            os.system("mkdir -p %s %s/jobs/" %(ANA_ARGS.logsdir, ANA_ARGS.outdir))
            for aw in ana_workers:
                jfile_name = "%s/jobs/%s.pbs" % (ANA_ARGS.outdir, aw.name)

                # - - write the job submission script with right permissions
                jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                os.write(jfile,
                         PBS_JOB_TEMPLATE.format(logsdir=ANA_ARGS.logsdir,
                                                 outdir=ANA_ARGS.outdir,
                                                 local_scratch=ANA_ARGS.local_scratch,
                                                 pickled_analysis=ANA_ARGS.pickled_analysis,
                                                 script_path=os.path.join(__HERE, "process-dataset"),
                                                 jobname=aw.name,))

                os.close(jfile)

                # - - - - submit the job now
                log.info("submitting  %s ..." % aw.name)
                if not ANA_ARGS.dry_run:
                    os.system("qsub {0}".format(jfile_name))

        elif ANA_ARGS.rs_manager == "SLURM":
            # - - write jobs
            from hpana.cluster.job_template import SLURM_JOB_TEMPLATE

            # - - setup the submit dir
            os.system(
                "mkdir -p {0}/jobs  {0}/logs {0}/hists".format(ANA_ARGS.outdir))
            log.info(
                "************** submitting %i jobs to the cluster ************" % len(ana_workers))
            log.info(
                "**************************************************************")

            os.system("mkdir -p %s %s/jobs/" %
                      (os.path.join(ANA_ARGS.outdir, ANA_ARGS.logsdir), ANA_ARGS.outdir))
            submitfile_name = "%s/%s.sh" % (ANA_ARGS.outdir, "submitAllJobs")
            submit_file = os.open(
                submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

            # - -  create the job scripts and run them. Returns Analysis workers
            for aw in ana_workers:
                if ANA_ARGS.dry_run:
                    print "--"*100
                    print aw.name

                # - - payload for container
                payload = "source {job_script} {pickled_analysis} {script_path}".format(
                    job_script=os.path.join(__HERE, "slurm_job.sh"),
                    pickled_analysis=ANA_ARGS.pickled_analysis,
                    script_path=os.path.join(__HERE, "process-dataset"),
                )

                # - - write the job submission script with right permissions
                jfile_name = "%s/jobs/%s.slurm" % (ANA_ARGS.outdir, aw.name)
                jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                os.write(jfile,
                         SLURM_JOB_TEMPLATE.format(
                             logsdir=os.path.join(
                                 ANA_ARGS.outdir, ANA_ARGS.logsdir),
                             outdir=ANA_ARGS.outdir,
                             local_scratch=ANA_ARGS.local_scratch,
                             jobname=aw.name,  #<! important - jobname steers which worker used from pickled ana
                             payload=payload,
                             project=ANA_ARGS.rs_project
                         )
                         )

                os.close(jfile)

                # - - - - write submission to file to be used outside container
                log.info("Writing to %s for job %s ..." %
                         (submitfile_name, aw.name))
                os.write(submit_file, "sbatch %s\n" % jfile_name)

            os.close(submit_file)
            log.warning("Submission not supported from within container.")
            log.warning("Please launch outside container via:")
            log.warning("\t sh {0}".format(submitfile_name))
    

    # - - @NOTE: if you submitted jobs to the cluster wait till they're all done!
    if ANA_ARGS.merge_hists:
        if ANA_ARGS.cluster:
            raise RuntimeError(
                "jobs are submitted to the cluster wait for them to get done")
        log.info(
            "************** merging histograms  ************")
        log.info(
            "***********************************************")
        analysis.merge_hists(
            histsdir=ANA_ARGS.outdir,
            overwrite=True,
            write=True,)


# -------------------------------------------------------------
# run
# -------------------------------------------------------------
if __name__ == "__main__":
    start_time = time.time()
    main()

    end_time = time.time()
    elapsed_time = (end_time - start_time)/60.
    log.info(
        "\n****************** elapsed time: %0.1f mins ******************" % elapsed_time)
