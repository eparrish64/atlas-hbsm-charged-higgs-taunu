#!/usr/bin/env python

import ROOT
from hpana import log
from hpana.categories import CUTFLOW
from hpana.analysis import Analysis
from hpana.config import Configuration
"""
* This script is meant to produce event counts either from a histograms file or 
* to calcualte them on the fly.
"""

# stdl-lib
import sys
import os
import time
from collections import namedtuple
from argparse import ArgumentParser

# PyIP
from tabulate import tabulate
import pandas as pd
from numpy import sqrt

# - - - - - - - - - parse yields args (needed before ROOT)
from hpana.cmd import get_yields_parser
yields_parser = get_yields_parser()
YIELDS_ARGS = yields_parser.parse_args()

# local

# - - - - time it
start_time = time.time()

# - - - - - - - - setup cmd args
log.setLevel(YIELDS_ARGS.log)

# - - - - - - - - setup ROOT
ROOT.gROOT.SetBatch(True)


# ---------------------------------------------------------------------------
# simple container class for yields
# ---------------------------------------------------------------------------
class Yield(namedtuple("Yields", "sample category systematic events error")):
    pass


# ---------------------------------------------------------------------------
# configure the analysis
# ---------------------------------------------------------------------------
config = Configuration(
    YIELDS_ARGS.channel,
    year=YIELDS_ARGS.year,
    mc_campaign=YIELDS_ARGS.mc_campaign,
    data_streams=YIELDS_ARGS.data_streams,
    db_version=YIELDS_ARGS.db_version,
    FFs_macros=["FFsCOM.cxx", "FFsCR.cxx"], 
    metTrigEff_macros=["metTrigEff.cxx"], 
    upsilon_macros=["CorrectUpsilon.cxx"],
)

# - - - - - - - - instantiate the analysis
analysis = Analysis(config, compile_cxx=True)

# - - - - - - - - categories & systematics
categories = config.categories 
if YIELDS_ARGS.categories:
    categories = filter(lambda c: c.name in YIELDS_ARGS.categories, categories+config.ff_cr_regions+config.clf_regions)

## systematics
# if YIELDS_ARGS.cutflow:
    # systematics = config.systematics[:1] #<! NOMINAL
# else:
all_systematics = config.systematics[:]  # <! common systematics
all_systematics += analysis.qcd.systematics[1:]  # <! QCD fakes only
if YIELDS_ARGS.systematics:
    systematics = filter(
        lambda s: s.name in YIELDS_ARGS.systematics, all_systematics)
elif YIELDS_ARGS.systs:
    systematics = all_systematics
else:
    systematics = config.systematics[:1] #<! NOMINAL

systematics_vars = []
for st in systematics:
    systematics_vars += st.variations 

## variable
fields = config.variables[:1]

## samples
samples = analysis.backgrounds + [analysis.data] + analysis.get_signals() 
backgrounds = analysis.backgrounds

if YIELDS_ARGS.samples:
    samples = filter(lambda s: s.name in YIELDS_ARGS.samples, samples)
    backgrounds = filter(lambda b: b.name in YIELDS_ARGS.samples, backgrounds)

## unblind data 
# analysis.data.blind_regions = []
# analysis.data.blind_streams = []

# ---------------------------------------------------------------------------
# get the yields
# ---------------------------------------------------------------------------
if YIELDS_ARGS.yields_table:
    yields = []

    # - - - - - - - - read from the histograms
    if YIELDS_ARGS.hists_file:
        hfile = ROOT.TFile(YIELDS_ARGS.hists_file, "READ")
        var = fields[0]
        for syst_var in systematics_vars:
            for sample in samples:
                for cat in categories:
                    hname = os.path.join(syst_var.name, config.hist_name_template.format(
                        sample.name, cat.name, var.name))
                    hist = hfile.Get(hname)
                    if not hist:
                        log.debug("can't find %s hist; using NOMINAL!" % hname)
                        hname = os.path.join("NOMINAL", config.hist_name_template.format(
                            sample.name, cat.name, var.name))

                    if sample.name=="Data" and cat.name in analysis.data.blind_regions:
                        events = -1
                    else:                       
                        hist = hfile.Get(hname)
                        if hist:
                            # events = hist.Integral(0, hist.GetNbinsX()+1)
                            thiserr = ROOT.Double(-999)
                            events =  hist.IntegralAndError(0, ( hist.GetNbinsX() + 1), thiserr, "")
                        else:
                            events = 0
                            thiserr=0
                    yields.append(
                        Yield(sample=sample.name, category=cat.name, systematic=syst_var.name, events=events, error=thiserr))

    # - - - - - - - - count on the fly
    else:
        log.info("calculating yields on the fly ...")
        y_hists = analysis.hists(
            samples=samples, fields=fields, categories=categories, systematics=systematics)

        yields = []
        for hs in y_hists:
            # <! include underflow/overflow bins
            # nevents = hs.hist.Integral(0, hs.hist.GetNbinsX()+1)

            thiserr = ROOT.Double(-999)
            nevents =  hs.hist.IntegralAndError(0, ( hs.hist.GetNbinsX() + 1), thiserr, "")
            yields.append(
                Yield(sample=hs.sample, category=hs.category, systematic=hs.systematic, events=nevents, error=thiserr))
                
    # - - - - - - - - sum of bkg
    if backgrounds:
        for cat in categories:
            yields_nom = filter(
                    lambda y: y.category == cat.name and y.systematic == "NOMINAL", yields)
            bkg_yields_nom = filter(
                    lambda y: y.sample in [b.name for b in backgrounds], yields_nom)

            events_sum_nom = sum([y.events for y in bkg_yields_nom])
            if events_sum_nom == 0:
                continue
            event_sum_err = sqrt(sum([y.error**2 for y in bkg_yields_nom]))

            for systematic in systematics_vars:
                yields_cat = filter(
                    lambda y: y.category == cat.name and y.systematic == systematic.name, yields)
                bkg_yields = filter(
                    lambda y: y.sample in [b.name for b in backgrounds], yields_cat)
                events_sum = sum([y.events for y in bkg_yields])
                events_err = sqrt(sum([y.error**2 for y in bkg_yields]))

                rvalue = "%0.2f +/- %1.2f "%(events_sum, events_err) if systematic.name=="NOMINAL" else "%0.2f +/ - %2.2f (%1.2f%%)"%(events_sum, (events_sum_nom/events_sum_nom)*100, events_err)
                # errvalue = "%0.2f"%events_sum if systematic.name=="NOMINAL" else "%0.2f(%0.2f%%)"%(events_sum, (events_sum_nom/events_sum_nom)*100)
                yields.append(
                    Yield(sample="bkgSum", category=cat.name, systematic=systematic.name, events=rvalue, error=events_err))

    # - - - - - - - - tabulate the yields
    # yieldDF = pd.DataFrame()
    allYields = []
    for cat in categories:
        ostring = "\n------------------------------ CATEGORY: %s ------------------------------\n\n" % cat.name
        yields_cat = filter(lambda y: y.category == cat.name, yields)

        headers = ["Systematic"]
        for y in yields_cat:
            if not y.sample in headers:
                headers += [y.sample]
        rows_cat = []
        for syst in [s.name for s in systematics_vars]:
            yields_cat_syst = filter(
                lambda y: y.systematic == syst, yields_cat)
            row = [syst[:50]] + [y.events for y in yields_cat_syst]
            rows_cat.append(row)
            allYields.append([cat.name]+row)

        # allYields.append(rows_cat)
        if YIELDS_ARGS.latex:
            ostring += tabulate(rows_cat, headers=headers, floatfmt=".2f", tablefmt='latex')
        else:
            ostring += tabulate(rows_cat, headers=headers, floatfmt=".2f")
        with open(YIELDS_ARGS.yfile, "a") as yfile:
            yfile.write(ostring)

        print ostring

    yieldDF = pd.DataFrame(allYields, columns=["Category"]+headers)
    yieldDF = yieldDF.round(2)
    print "\n"
    if YIELDS_ARGS.latex:
        print yieldDF.to_latex(index=False)
    else:
        pd.set_option("display.max_rows", None, "display.max_columns", None)
        print yieldDF

    print yieldDF["bkgSum"].to_string(index=False)
    yieldDF.to_csv('%s_%s_yields_1p_3p.csv' %(YIELDS_ARGS.channel,YIELDS_ARGS.db_version),index=False)

if YIELDS_ARGS.compare:
    yields1 = pd.read_csv("/afs/cern.ch/user/e/eparrish/workarea/public/HPlusTauNu/hpana_rnnTest/workAREA/taujet_rnnTest_yields_1p_3p.csv")
    yields2 = pd.read_csv("/afs/cern.ch/user/e/eparrish/workarea/public/HPlusTauNu/hpana_master/workAREA/taujet_v09g_yields_1p_3p.csv")
    yields1["Ntuple Version"] = "rnnTest"
    yields2["Ntuple Version"] = "v09"
    # print yields1.head()
    # print yields2.head()
    concatDF = yields1.append(yields2)
    concatDF.drop(columns=["Systematic"], inplace=True)
    # print concatDF.sort_values(["Category", "Ntuple Version"])
    finalDF = pd.DataFrame()

    group = concatDF.groupby(["Category"])
    # print group
    # print "AHH\n "
    diffDF = pd.DataFrame()
    for x in group:
        # print x[1]
        diffs = x[1][["TTbar","QCD","SingleTop","Wtaunu","LepFakes","Ztautau","DiBoson","Data","Hplus80","Hplus160","Hplus400","Hplus3000","bkgSum"]].pct_change(periods=1)*100
        diffs.dropna(axis=0,how="all",inplace=True)
        # diffs = diffs.round(0)
        # print type(diffs)
        # print diffs
        diffs["Category"] = x[0]
        diffs["Ntuple Version"] = "% Difference"
        finalDF = finalDF.append(x[1])
        finalDF = finalDF.append(diffs)
        diffDF = diffDF.append(diffs)
        # print diffs
        # for y in x:
        #     print y
        # y = x[1]
        # print y.append( (y.iloc[0] - y.iloc[1]) / y.iloc[0] , ignore_index=True)

    finalDF=finalDF[["Category","Ntuple Version","TTbar","QCD","SingleTop","Wtaunu","LepFakes","Ztautau","DiBoson","Data","Hplus80","Hplus160","Hplus400","Hplus3000","bkgSum"]]
    finalDF=finalDF.fillna(0)
    finalDF=finalDF.round(2)
    finalDF["Category"]=finalDF["Category"].str.lower()
    finalDF["Category"]=finalDF["Category"].str.replace("_"," ")
    print finalDF
    print finalDF.to_latex(index=False)


    diffDF=diffDF[["Category","Ntuple Version","TTbar","QCD","SingleTop","Wtaunu","LepFakes","Ztautau","DiBoson","Data","Hplus80","Hplus160","Hplus400","Hplus3000","bkgSum"]]
    diffDF=diffDF.fillna(0)
    diffDF=diffDF.round(2)
    diffDF["Category"]=diffDF["Category"].str.lower()
    diffDF["Category"]=diffDF["Category"].str.replace("_"," ")
    print diffDF
    print diffDF.to_latex(index=False)
    # finalDF.to_csv('v09_yields_3p.csv',index=False)

# ---------------------------------------------------------------------------
# get cutflow table
# ---------------------------------------------------------------------------
if YIELDS_ARGS.cutflow:
    cutflow_selections = CUTFLOW[YIELDS_ARGS.channel]
    table_rows = []
    cutflow_hist_sets = []
    bkg_sum_events = []
    bkg_sum_errs = []
    cutflow_hist_sets = analysis.cutflow(cutflow_selections, samples=samples, tauid=ROOT.TCut("1>0"), trigger=ROOT.TCut("1>0"), systematics=systematics, outdir=YIELDS_ARGS.outdir)

    for sample in samples:
        sample_hists = filter(lambda hs: hs.sample==sample.name, cutflow_hist_sets)

        # - - - - make sure the order is correct
        events = []
        errs = []
        for cname, cf in cutflow_selections.iteritems():
            for hs in sample_hists:
                if hs.hist==None:
                    continue
                if hs.category == cname:
                    thiserr = ROOT.Double(-999)
                    thisIntegral =  hs.hist.IntegralAndError(0, ( hs.hist.GetNbinsX() + 1), thiserr, "")
                    events.append(thisIntegral)
                    errs.append(thiserr)
                    # events.append(hs.hist.Integral(0, hs.hist.GetNbinsX()+1))

        if sample.name in [b.name for b in backgrounds]:
           bkg_sum_events += [events]
           bkg_sum_errs += [errs]

        # - - - - the the fraction of events failing
        s_fractions = [0]*len(cutflow_selections)
        for i in range(1, len(events)):
            if events[i-1] != 0:
                s_fractions[i] = events[i]/(events[i-1])
            else:
                s_fractions[i] = 0

        evt_fract = []
        for evt, frac, err in zip(events, s_fractions, errs):
            if frac != 0:
                evt_fract.append("{0:0.2f} +/- {1:0.2f} ({2:0.0f}%)".format(evt, err, 100*frac))
            else:
                evt_fract.append("{0:0.2f} +/- {1:0.2f}".format(evt, err))
        row = [sample.name] + evt_fract
        table_rows.append(row)

    ## bkg sum 
    if len(bkg_sum_events)>0:
        bkg_tot = []
        bkg_tot_errs = []
        for i in range(len(bkg_sum_events[0])):
            br = 0
            br_err  = 0
            for be in bkg_sum_events:
                br += be[i]
            bkg_tot += [br]

            for be_err in bkg_sum_errs:
                br_err += be_err[i]**2
            bkg_tot_errs += [br_err]


        ## the the fraction of events failing
        s_fractions = [0]*len(cutflow_selections)
        for i in range(1, len(bkg_tot)):
            if bkg_tot[i-1] != 0:
                s_fractions[i] = bkg_tot[i]/(bkg_tot[i-1])
            else:
                s_fractions[i] = 0

        evt_fract = []
        for evt, frac, err in zip(bkg_tot, s_fractions, bkg_tot_errs):
            if frac != 0:
                evt_fract.append("{0:0.2f} +/- {1:0.2f} ({2:0.0f}%)".format(evt, sqrt(err), 100*frac))
            else:
                evt_fract.append("{0:0.2f} +/- {1:0.2f}".format(evt, sqrt(err)))
        row = ["Total bkg"] + evt_fract
        table_rows.append(row)

    # - - - - format the table
    headers = ["sample"] + cutflow_selections.keys()
    ostring = "--"*100 + "\n"
    if YIELDS_ARGS.latex:
        ostring += tabulate(table_rows, headers=headers, tablefmt='latex')
    else:
        ostring += tabulate(table_rows, headers=headers)
    ostring += "\n" + "--"*100 + "\n"

    # - - - - save the table
    with open(YIELDS_ARGS.yfile, "a") as yfile:
        yfile.write(ostring)
    print ostring


end_time = time.time()
elapsed_time = (end_time - start_time)/60.
log.info("\n****************** elapsed time: %0.1f mins ******************" %
         elapsed_time)
