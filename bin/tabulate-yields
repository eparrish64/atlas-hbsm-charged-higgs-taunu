#!/usr/bin/env python

import ROOT
from hpana import log
from hpana.categories import CUTFLOW
from hpana.analysis import Analysis
from hpana.config import Configuration
"""
* This script is meant to produce event counts either from a histograms file or 
* to calcualte them on the fly.
"""

# stdl-lib
import sys
import os
import time
from collections import namedtuple
from argparse import ArgumentParser

# PyIP
from tabulate import tabulate

# - - - - - - - - - parse yields args (needed before ROOT)
from hpana.cmd import get_yields_parser
yields_parser = get_yields_parser()
YIELDS_ARGS = yields_parser.parse_args()

# local

# - - - - time it
start_time = time.time()

# - - - - - - - - setup cmd args
log.setLevel(YIELDS_ARGS.log)

# - - - - - - - - setup ROOT
ROOT.gROOT.SetBatch(True)


# ---------------------------------------------------------------------------
# simple container class for yields
# ---------------------------------------------------------------------------
class Yield(namedtuple("Yields", "sample category systematic events")):
    pass


# ---------------------------------------------------------------------------
# configure the analysis
# ---------------------------------------------------------------------------
config = Configuration(
    YIELDS_ARGS.channel,
    year=YIELDS_ARGS.year,
    mc_campaign=YIELDS_ARGS.mc_campaign,
    data_streams=YIELDS_ARGS.data_streams,
    db_version=YIELDS_ARGS.db_version)

# - - - - - - - - instantiate the analysis
analysis = Analysis(config, compile_cxx=True)

# - - - - - - - - categories & systematics
categories = config.categories
if YIELDS_ARGS.categories:
    categories = filter(lambda c: c.name in YIELDS_ARGS.categories, categories)

## systematics
all_systematics = config.systematics[:]  # <! common systematics
all_systematics += analysis.qcd.systematics[1:]  # <! QCD fakes only
if YIELDS_ARGS.systematics:
    systematics = filter(
        lambda s: s.name in YIELDS_ARGS.systematics, all_systematics)
elif YIELDS_ARGS.systs:
    systematics = all_systematics
else:
    systematics = config.systematics[:1] #<! NOMINAL

systematics_vars = []
for st in systematics:
    systematics_vars += st.variations 

fields = config.variables[0:1]  # <! tau_0_pt

# - - - - - - - - samples
samples = analysis.backgrounds + [analysis.data] #+ analysis.signals 
backgrounds = analysis.backgrounds
if YIELDS_ARGS.samples:
    samples = filter(lambda s: s.name in YIELDS_ARGS.samples, samples)
    backgrounds = filter(lambda b: b.name in YIELDS_ARGS.samples, backgrounds)

# ---------------------------------------------------------------------------
# get the yields
# ---------------------------------------------------------------------------
if YIELDS_ARGS.yields_table:
    yields = []

    # - - - - - - - - read from the histograms
    if YIELDS_ARGS.hists_file:
        hfile = ROOT.TFile(YIELDS_ARGS.hists_file, "READ")
        var = fields[0]
        for syst_var in systematics_vars:
            for sample in samples:
                for cat in categories:                    
                    hname = os.path.join(syst_var.name, config.hist_name_template.format(
                        sample.name, cat.name, var.name))
                    hist = hfile.Get(hname)
                    if not hist:
                        log.debug("can't find %s hist; using NOMINAL!" % hname)
                        hname = os.path.join("NOMINAL", config.hist_name_template.format(
                            sample.name, cat.name, var.name))

                    if sample.name=="Data" and cat.name in analysis.data.blind_regions:
                        events = -1
                    else:                        
                        hist = hfile.Get(hname)
                        events = hist.Integral(0, -1)
                    yields.append(
                        Yield(sample=sample.name, category=cat.name, systematic=syst_var.name, events=events))

    # - - - - - - - - count on the fly
    else:
        log.info("calculating yields on the fly ...")
        y_hists = analysis.hists(
            samples=samples, fields=fields, categories=categories, systematics=systematics)

        yields = []
        for hs in y_hists:
            # <! include underflow/overflow bins
            nevents = hs.hist.Integral(0, -1)
            yields.append(
                Yield(sample=hs.sample, category=hs.category, systematic=hs.systematic, events=nevents))
                
    # - - - - - - - - sum of bkg
    if backgrounds:
        for cat in categories:
            yields_nom = filter(
                    lambda y: y.category == cat.name and y.systematic == "NOMINAL", yields)
            bkg_yields_nom = filter(
                    lambda y: y.sample in [b.name for b in backgrounds], yields_nom)
            events_sum_nom = sum([y.events for y in bkg_yields_nom])

            for systematic in systematics_vars:
                yields_cat = filter(
                    lambda y: y.category == cat.name and y.systematic == systematic.name, yields)
                bkg_yields = filter(
                    lambda y: y.sample in [b.name for b in backgrounds], yields_cat)
                events_sum = sum([y.events for y in bkg_yields])
                rvalue = "%0.2f"%events_sum if systematic.name=="NOMINAL" else "%0.2f(%0.2f%%)"%(events_sum, ((events_sum-events_sum_nom)/events_sum_nom)*100)
                yields.append(
                    Yield(sample="bkgSum", category=cat.name, systematic=systematic.name, events=rvalue))

    # - - - - - - - - tabulate the yields
    for cat in categories:
        ostring = "\n------------------------------ CATEGORY: %s ------------------------------\n\n" % cat.name
        yields_cat = filter(lambda y: y.category == cat.name, yields)

        headers = ["Systematic"]
        for y in yields_cat:
            if not y.sample in headers:
                headers += [y.sample]
        rows_cat = []
        for syst in [s.name for s in systematics_vars]:
            yields_cat_syst = filter(
                lambda y: y.systematic == syst, yields_cat)
            row = [syst[:50]] + [y.events for y in yields_cat_syst]
            rows_cat.append(row)

        if YIELDS_ARGS.latex:
            ostring += tabulate(rows_cat, headers=headers, floatfmt=".2f", tablefmt='latex')
        else:
            ostring += tabulate(rows_cat, headers=headers, floatfmt=".2f")
        with open(YIELDS_ARGS.yfile, "a") as yfile:
            yfile.write(ostring)

        print ostring

# ---------------------------------------------------------------------------
# get cutflow table
# ---------------------------------------------------------------------------
if YIELDS_ARGS.cutflow:
    cutflow_selections = CUTFLOW[YIELDS_ARGS.channel]
    table_rows = []
    cutflow_hist_sets = []

    cutflow_hist_sets = analysis.cutflow(
        cutflow_selections, samples=samples, tauid=ROOT.TCut("1>0"), trigger=ROOT.TCut("1>0"))

    for sample in samples:
        sample_hists = filter(lambda hs: hs.sample ==
                              sample.name, cutflow_hist_sets)

        # - - - - make sure the order is correct
        events = []
        for cname, cf in cutflow_selections.iteritems():
            for hs in sample_hists:
                if hs.category == cname:
                    events.append(hs.hist.Integral(0, hs.hist.GetNbinsX()+1))

        # - - - - the the fraction of events failing
        s_fractions = [0]*len(cutflow_selections)
        for i in range(1, len(events)):
            if events[i-1] != 0:
                s_fractions[i] = (events[i-1] - events[i])/(events[i-1])
            else:
                s_fractions[i] = 0

        evt_fract = []
        for evt, frac in zip(events, s_fractions):
            if frac != 0:
                evt_fract.append("{0:0.2f} ({1:0.0f}%)".format(evt, 100*frac))
            else:
                evt_fract.append("{0:0.2f}".format(evt))
        row = [sample.name] + evt_fract
        table_rows.append(row)

    # - - - - format the table
    headers = ["sample"] + cutflow_selections.keys()
    ostring = "--"*50 + "\n"
    if YIELDS_ARGS.latex:
        ostring += tabulate(table_rows, headers=headers, tablefmt='latex')
    else:
        ostring += tabulate(table_rows, headers=headers)
    ostring += "\n" + "--"*50 + "\n"

    # - - - - save the table
    with open(YIELDS_ARGS.yfile, "a") as yfile:
        yfile.write(ostring)
    print ostring


end_time = time.time()
elapsed_time = (end_time - start_time)/60.
log.info("\n****************** elapsed time: %0.1f mins ******************" %
         elapsed_time)
