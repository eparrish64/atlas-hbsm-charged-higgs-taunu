#!/usr/bin/env python
"""
* This scripts provides functionalities for training a binary classifier in order to separate signal from background.
* It's build on top of the MVA with the option for Pythonic keras & scikit-learn backends.
"""

## stdlib
import os, sys, glob, multiprocessing
from subprocess import call
from os import environ

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_clf_parser 
clf_parser = get_clf_parser()
CLF_ARGS = clf_parser.parse_args()

## PyPI
from root_numpy import root2array, tree2array
import h5py
import numpy as np
import pickle, cPickle

## local
from hpana.config import Configuration
from hpana.analysis import Analysis
from hpana.samples.sample  import Sample
from hpana.samples.higgs import Higgs 
from hpana.variables import CLF_FEATURES
from hpana.categories import CLASSIFIER_CATEGORIES
from hpana.cluster.parallel import run_pool, Job

from hpana.mva.classifier import Classifier, SClassifier, train_model
from hpana.mva.optimization import get_hparams
from hpana.mva import N_TRACKS, FOLD_CUT_STR, TRAINING_MASS_BINS, BRANCHES, ALL_FEATS

## - - set log level
from hpana import log
log.setLevel(CLF_ARGS.log)

# - - speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")

##-------------------------------------------------------------
## consts 
##-------------------------------------------------------------
__HERE = os.path.dirname(os.path.abspath(__file__))

## - - build analysis main configuration object
CONFIG = Configuration(CLF_ARGS.channel, db_version=CLF_ARGS.db_version, data_streams=CLF_ARGS.data_streams,)

## - - instantiate Analysis
ANALYSIS  = Analysis(CONFIG, compile_cxx=True)

## - - training samples (no need for l-->tau fakes or MC j--> tau fakes for masses < 500GeV)
BKGS = filter(lambda b: not b.name in ["LepFakes"], ANALYSIS.backgrounds)
if CLF_ARGS.bkg:
    BKGS = filter(lambda b: b.name in CLF_ARGS.bkg, BKGS)

SIGS = ANALYSIS.get_signals()
if CLF_ARGS.sig:
    SIGS = filter(lambda s: s.name in CLF_ARGS.sig, SIGS)

## different set of bkgs for different signals regions 
TRAINING_BKGS = { 
    "LOW": filter(lambda b: not b.name in ["QCD"] , BKGS),
    "HIGH":  BKGS[:], #<! above 500 GeV
}  

## - - setup outdir
os.system("mkdir -p %s"%CLF_ARGS.outdir)
    

##-----------------------------
## sklearn backend 
##-----------------------------
if CLF_ARGS.backend=="sklearn":
    try:
        import sklearn
    except ImportError:
        raise RuntimeError("Please install sklearn; (pip install sklearn)")

    ## - - prepare training Dataframe
    DFRAME = SClassifier.prepare_data(BKGS, SIGS, ALL_FEATS[CLF_ARGS.channel], data_lumi=CONFIG.data_lumi,
                                      channel=CLF_ARGS.channel, branches=BRANCHES[CLF_ARGS.channel], train_data=CLF_ARGS.train_data)
    jobs = []
    models = []
    train_masses = TRAINING_MASS_BINS[CLF_ARGS.bin_scheme]
    for signal_masses in train_masses:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
        signals = filter(lambda s: s.mass in signal_masses, SIGS)
        if CLF_ARGS.mass_range:
            signals = filter(lambda s:int(CLF_ARGS.mass_range[0]) <= s.mass <= int(CLF_ARGS.mass_range[1]), signals)   
        if not signals:
            log.info("No signal in %s mass range; skipping!"%mass_tag)
            continue

        ## - - different set of features for low mass and high mass
        if (len(signals)==1 and signals[0].mass > 400) or any([s.mass>500 for s in signals]):
            feats = CLF_FEATURES[CLF_ARGS.channel]["HIGH"]
        else:
            feats = CLF_FEATURES[CLF_ARGS.channel]["LOW"]
        
        ## get tuned Hyperparameters
        hyper_params = get_hparams(CLF_ARGS.channel, mass_range=signal_masses, bin_scheme=CLF_ARGS.bin_scheme, model_type="GB")
        hyper_params["verbose"] = 1

        ## get list of backgrounds to be used in this signal region 
        if max(signal_masses) > 400:
            reg_bkgs = TRAINING_BKGS["HIGH"]
        else:
            reg_bkgs = TRAINING_BKGS["LOW"]

        ## retrive Dataframe     
        dframe = DFRAME.loc[[bkg.name for bkg in reg_bkgs]+[sig.name for sig in signals]]
        s_dframe = DFRAME.loc[[sig.name for sig in signals]]
        log.info("*"*80)
        log.info("Signals: {0} | #events: {1} ; backgrounds: {2} | #events: {3}".format(
            [s.name for s in signals], s_dframe.shape[0], [b.name for b in reg_bkgs], dframe.shape[0]-s_dframe.shape[0]))
        log.debug(30*"*" + " Training Data Frame " + 30*"*")
        log.debug(dframe)

        ## training separately for 1p and 3p taus or inclusively(do not include QCD fakes for 1p training)
        if CLF_ARGS.inc_trks:
            ntracks = 13 #<! a dummy value for consistency 
            for rem in range(CLF_ARGS.kfolds): #<@NOTE 1 obviously is all
                where = ""
                if CLF_ARGS.kfolds>1:
                    where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem)
                sdf = dframe[where]
                                    
                log.info("Training in mass range:%s on ntracks=%i, nevents=%i, and nfeatures=%i\n\t\tHyperparams: %r\n"%(
                    mass_tag, ntracks, sdf.shape[0], sdf.shape[1]-4, hyper_params))

                ## - - instantiate the model
                clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                    "GB%i"%hyper_params["n_estimators"],CLF_ARGS.channel, mass_tag, ntracks, CLF_ARGS.kfolds, rem, len(feats))
                sclf = SClassifier(CLF_ARGS.channel, 
                    train_df=sdf, name=clf_name, features=feats, weight_sample=False, mass_range=signal_masses, kfolds=CLF_ARGS.kfolds, fold_num=rem, **hyper_params)
                models += [sclf]

                ## - - assign the job
                if CLF_ARGS.train_bdt and CLF_ARGS.parallel:
                    jobs.append(Job(train_model, sclf, weight_sample=False, balanced=True, scale_features=False, outdir=CLF_ARGS.outdir))

                ## pickle the model                        
                mpath = os.path.join(CLF_ARGS.outdir, sclf.name)
                if os.path.isfile(mpath):
                    log.warning("Found %s model; skipping the preparation..."%mpath)
                    continue
                else:
                    log.info("Saving %s model to disk ..."%mpath)
                    with open(mpath, "w") as mcache:
                        cPickle.dump(sclf, mcache, protocol=2)
        else:
            for ntracks in N_TRACKS:
                for rem in range(CLF_ARGS.kfolds): #<@NOTE 1 obviously is all
                    if CLF_ARGS.kfolds==1:
                        where = (dframe["tau_0_n_charged_tracks"]==ntracks)
                    else:
                        where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem) & (dframe["tau_0_n_charged_tracks"]==ntracks)
                    sdf = dframe[where]
                                        
                    log.info("Training in mass range:%s on ntracks=%i, nevents=%i, and nfeatures=%i\n\t\tHyperparams: %r\n"%(
                        mass_tag, ntracks, sdf.shape[0], sdf.shape[1]-4, hyper_params))

                    ## - - instantiate the model
                    clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                        "GB%i"%hyper_params["n_estimators"],CLF_ARGS.channel, mass_tag, ntracks, CLF_ARGS.kfolds, rem, len(feats))
                    sclf = SClassifier(CLF_ARGS.channel, 
                        train_df=sdf, name=clf_name, features=feats, weight_sample=False, mass_range=signal_masses, kfolds=CLF_ARGS.kfolds, fold_num=rem, **hyper_params)
                    models += [sclf]

                    ## - - assign the job
                    if CLF_ARGS.train_bdt and CLF_ARGS.parallel:
                        jobs.append(Job(train_model, sclf, weight_sample=False, balanced=True, scale_features=False, outdir=CLF_ARGS.outdir))

                    ## pickle the model                        
                    mpath = os.path.join(CLF_ARGS.outdir, sclf.name)
                    if os.path.isfile(mpath):
                        log.warning("Found %s model; skipping the preparation..."%mpath)
                        continue
                    else:
                        log.info("Saving %s model to disk ..."%mpath)
                        with open(mpath, "w") as mcache:
                            cPickle.dump(sclf, mcache, protocol=2)
                                    
    
    if CLF_ARGS.train_bdt:
        ## - - run
        log.info("**"*30 + " Submitting %i jobs "%len(jobs) + "**"*30)
        if CLF_ARGS.parallel:
            run_pool(jobs, n_jobs=-1)

        if CLF_ARGS.cluster:
            if CLF_ARGS.rs_manager=="TORQUE":
                raise RuntimeError("Not implemented yet ...")

            elif CLF_ARGS.rs_manager == "SLURM":
                # - - write jobs
                from hpana.cluster.job_template import SLURM_JOB_TEMPLATE

                # - - setup the submit dir
                os.system(
                    "mkdir -p {0}/jobs  {0}/logs {0}/models".format(CLF_ARGS.outdir))
                log.info(
                    "************** submitting %i jobs to the cluster ************" % len(models))
                log.info(
                    "**************************************************************")

                os.system("mkdir -p %s %s/jobs/" %
                        (os.path.join(CLF_ARGS.outdir, CLF_ARGS.logsdir), CLF_ARGS.outdir))
                submitfile_name = "%s/%s.sh" % (CLF_ARGS.outdir, "submitAllJobs")
                submit_file = os.open(
                    submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

                # - -  create the job scripts and run them. Returns Analysis workers
                for model in models:
                    # - - payload for container
                    payload = "source {job_script} {pickled_model} {script_path}".format(
                        job_script=os.path.join(__HERE, "slurm_job_clf.sh"),
                        pickled_model=os.path.abspath(os.path.join(CLF_ARGS.outdir, model.name)),
                        script_path=os.path.join(__HERE, script),
                    )

                    # - - write the job submission script with right permissions
                    jfile_name = "%s/jobs/%s.slurm" % (CLF_ARGS.outdir, model.name)
                    jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                    os.write(jfile,
                            SLURM_JOB_TEMPLATE.format(
                                logsdir=os.path.join(CLF_ARGS.outdir, CLF_ARGS.logsdir),
                                outdir=CLF_ARGS.outdir,
                                local_scratch=CLF_ARGS.local_scratch,
                                time="6:0:0",
                                cores=1,
                                memory="1GB",
                                jobname=model.name, 
                                payload=payload,
                                project=CLF_ARGS.rs_project)
                            )

                    os.close(jfile)

                    # - - - - write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                            (submitfile_name, model.name))
                    os.write(submit_file, "sbatch %s\n" % jfile_name)

                os.close(submit_file)
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t sh {0}".format(submitfile_name))


        
##-----------------------------
## tmva backend
##-----------------------------
else:
    ## - - initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
    for signal_masses in MASS_RANGES:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
        if CLF_ARGS.train_bdt:
            clf_name = "BDT_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kBDT,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
                "method_params": "!H:!V:NTrees=100:MaxDepth=10:BoostType=AdaBoost:nCuts=20",
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    bdt_classifier = Classifier(**clf_params)
                    bdt_classifier.train(backgrounds, signals, **tr_params)
            else:
                ## - - instantite classifier
                bdt_classifier = Classifier(**clf_params)
                bdt_classifier.train(backgrounds, signals, **tr_params)


        ##-----------------------------
        ## train Neural Networks
        ##-----------------------------
        if CLF_ARGS.train_nn:
            # Select Theano as backend for Keras
            environ['KERAS_BACKEND'] = 'theano'

            # Set architecture of system (AVX instruction set is not supported on SWAN)
            environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'
            from keras.models import Sequential
            from keras.layers import Dense

            clf_name = "NN_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kPyKeras,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    model = Sequential()
                    model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                    model.add(Dense(2, activation='softmax'))
                    model.summary()
                    model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                    model_name = "%s/%s_model_mod_%i_rem_%i.h5"%(CLF_ARGS.outdir, clf_name, CLF_ARGS.kfolds, rem)
                    model.save(model_name)
                    method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    nn_classifier = Classifier(**clf_params)
                    nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)
            else:
                model = Sequential()
                model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                model.add(Dense(2, activation='softmax'))
                model.summary()
                model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                model_name = "%s/%s_model.h5"%(CLF_ARGS.outdir, clf_name)
                model.save(model_name)
                method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                ## - - instantite classifier
                nn_classifier = Classifier(**clf_params)
                nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)

