#!/usr/bin/env python
"""
* This scripts provieds functionalities for trainning a binary classifier in order to separate signal from background.
* It's build on top of the MVA with the option for Pythonic keras & scikit-learn backends.
"""

## stdlib
import os, sys, glob, multiprocessing
from subprocess import call
from os import environ

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_clf_parser 
clf_parser = get_clf_parser()
CLF_ARGS = clf_parser.parse_args()

## PyPI
from root_numpy import root2array, tree2array
import h5py
import numpy as np
import pickle, cPickle

## local
from hpana.config import Configuration
from hpana.analysis import Analysis
from hpana.samples.sample  import Sample
from hpana.samples.higgs import Higgs 
from hpana.variables import CLF_FEATURES
from hpana.classifier import Classifier, SClassifier, train_model, optimize_model
from hpana.categories import CLASSIFIER_CATEGORIES
from hpana.cluster.parallel import run_pool, Job
from hpana import log

## - - set log level
log.setLevel(CLF_ARGS.log)

# - - speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")

##-------------------------------------------------------------
## consts 
##-------------------------------------------------------------
TREE_NAME = "NOMINAL"
N_TRACKS = [1]
FOLD_CUT_STR = "event_number%{0}!={1}"

## different training mass bins 
TRAINING_MASS_BINS = {
    # as of 2015+2016 legacy analysis
    "NOM": [     
        (80, 90, 100, 110, 120), #< low mas 
        (130, 140, 150, 160), #< int mass I
        (170, 180, 190), #< int mass II
        (200, 225, 250, 275, 300, 350, 400), #< high mass I
        tuple(range(500, 1100, 100) + range(1200, 2200, 200) + [2500, 3000]), #< high mass II
        ],   
    # one mass point above and one blow --> no discontinuity in training bins 
    "UP_DOWN": [(80, 90, 100), (2000, 2500, 3000)] + [tuple(Higgs.MASSES[cnt-1:cnt+2]) for cnt in range(len(Higgs.MASSES))[3:-2]],

    # train per mass point 
    "SINGLE": [(m,) for m in Higgs.MASSES],
} 

## - - build analysis main configuration object
CONFIG = Configuration(CLF_ARGS.channel, 
                       data_streams=CLF_ARGS.data_streams,
                       mc_campaign=CLF_ARGS.mc_campaign,
                       db_version=CLF_ARGS.db_version)

## - - instantiate Analysis
ANALYSIS  = Analysis(CONFIG, compile_cxx=True)

## - - training samples (no need for l-->tau fakes or MC j--> tau fakes for masses < 500GeV)
BKGS = filter(lambda b: not b.name in ["LepFakes"] , ANALYSIS.backgrounds)
if CLF_ARGS.bkg:
    BKGS = filter(lambda b: b.name in CLF_ARGS.bkg, BKGS)

SIGS = ANALYSIS.get_signals()
if CLF_ARGS.sig:
    SIGS = filter(lambda s: s.name in CLF_ARGS.sig, SIGS)

## - - complete list of training features irrespective of the mass region (to get a complete Dataframe for training)
ALL_FEATS = []
for region, rg_feats in CLF_FEATURES[CLF_ARGS.channel].iteritems():
    for ft in rg_feats:
        if not ft.name in [f.name for f in ALL_FEATS]:
            ALL_FEATS += [ft] 

## - - setup outdir
os.system("mkdir -p %s"%CLF_ARGS.outdir)
    
## - - Hyperparameters for GradientBoosting 
GB_HYPERPARAMS = {
    # "loss": ["deviance", "exponential"],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "n_estimators":[50, 100, 150, 200, 300, 400], 
    "min_samples_leaf": [0.005, 0.01, 0.02],
    "max_depth": [10, 15, 20],
}

##-----------------------------
## sklearn backend 
##-----------------------------
if CLF_ARGS.backend=="sklearn":
    try:
        import sklearn
    except ImportError:
        raise RuntimeError("Please install sklearn; (pip install sklearn)")

    ## - - turn off kfold splitting
    if CLF_ARGS.plot_correlations or CLF_ARGS.optimize_bdt:
        CLF_ARGS.kfolds = None

    ## - - event number for kfold training and ntracks for different training for different tau prongs. 
    BRACHES = ["event_number"] + ["tau_0_n_charged_tracks"] + [ft.tformula for ft in ALL_FEATS]

    ## nominal prams for GB classifier 
    mparams = {
        "n_estimators":200, 
        "min_weight_fraction_leaf": 0.02,
        "max_depth": 10,
        "verbose":1,
        "learning_rate":0.1,
        }
    clf_type = "GB%i"%mparams["n_estimators"]

    ## - - prepare training Dataframe
    DFRAME = SClassifier.prepare_data(BKGS, SIGS, ALL_FEATS, data_lumi=CONFIG.data_lumi,
                                      channel=CLF_ARGS.channel, branches=BRACHES, train_data=CLF_ARGS.train_data)
    jobs = []
    train_masses = TRAINING_MASS_BINS["NOM"]
    if CLF_ARGS.per_mass:
        train_masses = TRAINING_MASS_BINS["SINGLE"]

    for signal_masses in train_masses:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
        signals = filter(lambda s: s.mass in signal_masses, SIGS)
        if CLF_ARGS.mass_range:
            signals = filter(lambda s:int(CLF_ARGS.mass_range[0]) <= s.mass <= int(CLF_ARGS.mass_range[1]), signals)   
        if not signals:
            log.info("No signal in %s mass range; skipping!"%mass_tag)
            continue
        dframe = DFRAME.loc[[bkg.name for bkg in BKGS]+[sig.name for sig in signals]]
        s_dframe = DFRAME.loc[[sig.name for sig in signals]]
        log.info("*"*80)
        log.info("Signals: {0} || #events: {1} ; backgrounds: {2} || #events: {3}".format(
            [s.name for s in signals], s_dframe.shape[0], [b.name for b in BKGS], dframe.shape[0]-s_dframe.shape[0]))
        log.debug(30*"*" + " Training Data Frame " + 30*"*")
        log.debug(dframe)

        ## training separately for 1p and 3p taus (do not include QCD fakes for 1p training)
        for ntracks in N_TRACKS:
            if CLF_ARGS.kfolds:
                for rem in range(CLF_ARGS.kfolds):
                    sdf = dframe[(dframe["event_number"]%CLF_ARGS.kfolds!=rem) & (dframe["tau_0_n_charged_tracks"]==ntracks)]

                    ## - - different set of features for low mass and high mass
                    if (len(signals)==1 and signals[0].mass > 400) or any([s.mass>500 for s in signals]):
                        feats = CLF_FEATURES[CLF_ARGS.channel]["HIGH"]
                    else:
                        feats = CLF_FEATURES[CLF_ARGS.channel]["LOW"]

                    ## - - training arrays
                    X_train = sdf[[ft.name for ft in feats]]    
                    Y_train = sdf["class_label"]
                    X_weight = sdf["weight"] #<! pass it sample_weight to the fit method
                    
                    log.info("Training in mass range %s on ntracks=%i, nfold=%i, nevents=%i, and nfeatures=%i"%(
                        mass_tag, ntracks, rem, X_train.shape[0], X_train.shape[1]))

                    ## - - instantiate the model
                    clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                        clf_type, CLF_ARGS.channel, mass_tag, ntracks, CLF_ARGS.kfolds, rem, X_train.shape[1])
                    sclf = SClassifier(CLF_ARGS.channel, 
                    name=clf_name, features=feats, X_train=X_train, X_weight=X_weight, Y_train=Y_train, weight_sample=False, **mparams)

                    with open("tmpMODEL.pkl", "w") as ml:
                        cPickle.dump(sclf, ml, protocol=2)

                    with open("tmpMODEL.pkl", "r") as ml:
                        tr_ml = cPickle.load(ml)

                    ## - - assign the job
                    if CLF_ARGS.train_bdt:
                        jobs.append(Job(train_model, tr_ml, X_train, Y_train, X_weight,
                                        weight_sample=True, outdir=CLF_ARGS.outdir))
            else:
                sdf = dframe[(dframe["tau_0_n_charged_tracks"]==ntracks)]
                ## - - different set of features for low mass and high mass
                if (len(signals)==1 and signals[0].mass > 400) or any([s.mass>500 for s in signals]):
                    feats = CLF_FEATURES[CLF_ARGS.channel]["HIGH"]
                else:
                    feats = CLF_FEATURES[CLF_ARGS.channel]["LOW"]

                ## - - training arrays
                X_train = sdf[[ft.name for ft in feats]]    

                Y_train = sdf["class_label"]
                X_weight = sdf["weight"] #<! pass it sample_weight to the fit method
                
                log.info("Training in mass rage:%s on ntracks=%i, nevents=%i, and nfeatures=%i"%(
                    mass_tag, ntracks, X_train.shape[0], X_train.shape[1]))

                ## - - instantiate the model
                clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                    clf_type, CLF_ARGS.channel, mass_tag, ntracks, "NA", "NA", X_train.shape[1])
                sclf = SClassifier(CLF_ARGS.channel, 
                    name=clf_name, features=feats, X_train=X_train, X_weight=X_weight, Y_train=Y_train, weight_sample=False, **mparams)

                ## - - assign the job
                if CLF_ARGS.train_bdt:
                    jobs.append(Job(train_model, sclf, X_train, Y_train, X_weight,
                                    weight_sample=True, outdir=CLF_ARGS.outdir))
                
            ## correlation matrix ?
            if CLF_ARGS.plot_correlations:
                from hpana.classifier import features_correlation
                p_title = r"Correlation Matrix: $ %i \leq m_{H^+} \leq %i [GeV]$"%(signal_masses[0], signal_masses[-1])
                features_correlation(X_train, CLF_FEATURES[CLF_ARGS.channel], outname=clf_name.replace(".pkl", ""), outdir=CLF_ARGS.outdir, title=p_title)

            if CLF_ARGS.optimize_bdt:
                jobs.append(Job(optimize_model, sclf, X_train, Y_train, X_weight, param_grid=GB_HYPERPARAMS,
                                weight_sample=False, outdir=CLF_ARGS.outdir, validation_plots=CLF_ARGS.validation_plots))
                                                        
    
    if CLF_ARGS.train_bdt or CLF_ARGS.optimize_bdt:
        ## - - run
        log.info("**"*30 + " Submitting %i jobs "%len(jobs) + "**"*30)
        if CLF_ARGS.parallel:
            run_pool(jobs, n_jobs=-1)
        else:
            run_pool(jobs, n_jobs=1)
        
##-----------------------------
## tmva backend
##-----------------------------
else:
    ## - - initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
    for signal_masses in MASS_RANGES:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
        if CLF_ARGS.train_bdt:
            clf_name = "BDT_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kBDT,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
                "method_params": "!H:!V:NTrees=100:MaxDepth=10:BoostType=AdaBoost:nCuts=20",
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    bdt_classifier = Classifier(**clf_params)
                    bdt_classifier.train(backgrounds, signals, **tr_params)
            else:
                ## - - instantite classifier
                bdt_classifier = Classifier(**clf_params)
                bdt_classifier.train(backgrounds, signals, **tr_params)


        ##-----------------------------
        ## train Neural Networks
        ##-----------------------------
        if CLF_ARGS.train_nn:
            # Select Theano as backend for Keras
            environ['KERAS_BACKEND'] = 'theano'

            # Set architecture of system (AVX instruction set is not supported on SWAN)
            environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'
            from keras.models import Sequential
            from keras.layers import Dense

            clf_name = "NN_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kPyKeras,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    model = Sequential()
                    model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                    model.add(Dense(2, activation='softmax'))
                    model.summary()
                    model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                    model_name = "%s/%s_model_mod_%i_rem_%i.h5"%(CLF_ARGS.outdir, clf_name, CLF_ARGS.kfolds, rem)
                    model.save(model_name)
                    method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    nn_classifier = Classifier(**clf_params)
                    nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)
            else:
                model = Sequential()
                model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                model.add(Dense(2, activation='softmax'))
                model.summary()
                model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                model_name = "%s/%s_model.h5"%(CLF_ARGS.outdir, clf_name)
                model.save(model_name)
                method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                ## - - instantite classifier
                nn_classifier = Classifier(**clf_params)
                nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)

