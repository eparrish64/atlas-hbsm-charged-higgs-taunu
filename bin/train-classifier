#!/usr/bin/env python
"""
* This scripts provides functionalities for training a binary classifier in order to separate signal from background.
* It's build on top of the MVA with the option for Pythonic keras & scikit-learn backends.
"""

## stdlib
import os, sys, glob, multiprocessing
from subprocess import call
from os import environ

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_clf_parser 
clf_parser = get_clf_parser()
# clf_parser.add_argument("--plot-sample-size", action="store_true")
CLF_ARGS = clf_parser.parse_args()

# print "\nThese are the arguments!!!!!!!!!!!!!!!!!!\n"
# print CLF_ARGS
# raise Exception("Debugging!!")

## PyPI
from root_numpy import root2array, tree2array
import h5py
import numpy as np
import pickle, cPickle
import pandas as pd

## local
from hpana.config import Configuration
from hpana.analysis import Analysis
from hpana.samples.sample  import Sample
from hpana.samples.higgs import Higgs 
from hpana.variables import CLF_FEATURES
from hpana.categories import CLASSIFIER_CATEGORIES
from hpana.cluster.parallel import run_pool, Job

from hpana.mva.classifier import Classifier, SClassifier, train_model
from hpana.mva.optimization import get_hparams, optimize_model
from hpana.mva import N_TRACKS, FOLD_CUT_STR, TRAINING_MASS_BINS, BRANCHES, ALL_FEATS, GB_HYPERPARAMS

## - - set log level
from hpana import log
log.setLevel(CLF_ARGS.log)

# - - speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")


##-------------------------------------------------------------
## consts 
##-------------------------------------------------------------
__HERE = os.path.dirname(os.path.abspath(__file__))

## - - build analysis main configuration object
CONFIG = Configuration(CLF_ARGS.channel, db_version=CLF_ARGS.db_version, data_streams=CLF_ARGS.data_streams,)

## - - instantiate Analysis
ANALYSIS  = Analysis(CONFIG, compile_cxx=True)

## - - training samples (no need for l-->tau fakes or MC j--> tau fakes for masses < 500GeV)
BKGS = filter(lambda b: not b.name in ["LepFakes"], ANALYSIS.backgrounds)
if CLF_ARGS.bkg:
    BKGS = filter(lambda b: b.name in CLF_ARGS.bkg, BKGS)

SIGS = ANALYSIS.get_signals()
if CLF_ARGS.sig:
    SIGS = filter(lambda s: s.name in CLF_ARGS.sig, SIGS)

## different set of bkgs for different signals regions 
TRAINING_BKGS = { 
    "LOW": filter(lambda b: not b.name in ["QCD"] , BKGS),
    "HIGH":  BKGS[:], #<! above 500 GeV
}  

## - - setup outdir
os.system("mkdir -p %s"%CLF_ARGS.outdir)
    

##-----------------------------
## sklearn backend 
##-----------------------------
if CLF_ARGS.backend=="sklearn":
    try:
        import sklearn
    except ImportError:
        raise RuntimeError("Please install sklearn; (pip install sklearn)")

    ## - - prepare training Dataframe
    DFRAME = SClassifier.prepare_data(BKGS, SIGS, ALL_FEATS[CLF_ARGS.channel], data_lumi=CONFIG.data_lumi,
                                      channel=CLF_ARGS.channel, branches=BRANCHES[CLF_ARGS.channel], train_data=CLF_ARGS.train_data)
    jobs = []
    models = []
    train_masses = TRAINING_MASS_BINS[CLF_ARGS.bin_scheme]
    # if CLF_ARGS.plot_sample_size:
    #     plot_sig_dist(DFRAME, signals=SIGS, outdir=CLF_ARGS.outdir)
    #     plot_bkg_dist(DFRAME, backgrounds=BKGS, outdir=CLF_ARGS.outdir)
    # print train_masses
    # ak
    i = 0
    # ak
    for signal_masses in train_masses:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"

        # if CLF_ARGS.bin_scheme=="UP_DOWN":
        # # ak
        print signal_masses[0],signal_masses[-1],signal_masses
        f_sig = 0
        l_sig = 0
        if (signal_masses[0] == 80):
            f_sig = signal_masses[0]
        else:
            f_sig = train_masses[i-1][0]

        if (signal_masses[0] == 3000):
            l_sig = signal_masses[0]
        else:
            l_sig = train_masses[i+1][0]
        
        print f_sig,l_sig
        
        signals = filter(lambda s:f_sig <= s.mass <= l_sig, SIGS)
        i = i +1
        # # ak

        # signals = filter(lambda s: s.mass in signal_masses, SIGS)


        if CLF_ARGS.mass_range:
            signals = filter(lambda s:int(CLF_ARGS.mass_range[0]) <= s.mass <= int(CLF_ARGS.mass_range[1]), signals)   
        if not signals:
            log.info("No signal in %s mass range; skipping!"%mass_tag)
            continue

        ## - - different set of features for low mass and high mass
        if (len(signals)==1 and signals[0].mass > 400) or any([s.mass>500 for s in signals]):
            feats = CLF_FEATURES[CLF_ARGS.channel]["HIGH"]
        else:
            feats = CLF_FEATURES[CLF_ARGS.channel]["LOW"]
        
        ## get tuned Hyperparameters
        hyper_params = get_hparams(CLF_ARGS.channel, mass_range=signal_masses, bin_scheme=CLF_ARGS.bin_scheme, model_type="GB")
        hyper_params["verbose"] = 1

        ## get list of backgrounds to be used in this signal region 
        if max(signal_masses) > 400:
            reg_bkgs = TRAINING_BKGS["HIGH"]
        else:
            reg_bkgs = TRAINING_BKGS["LOW"]

        ## retrive Dataframe     
        dframe = DFRAME.loc[[bkg.name for bkg in reg_bkgs]+[sig.name for sig in signals]]
        s_dframe = DFRAME.loc[[sig.name for sig in signals]]
        log.info("*"*80)
        log.info("Signals: {0} | #events: {1} ; backgrounds: {2} | #events: {3}".format(
            [s.name for s in signals], s_dframe.shape[0], [b.name for b in reg_bkgs], dframe.shape[0]-s_dframe.shape[0]))
        log.debug(30*"*" + " Training Data Frame " + 30*"*")
        log.debug(dframe)

        ## training separately for 1p and 3p taus or inclusively(do not include QCD fakes for 1p training)
        if CLF_ARGS.inc_trks:
            ntracks = 13 #<! a dummy value for consistency 
            for rem in range(CLF_ARGS.kfolds): #<@NOTE 1 obviously is all
                where = ""
                if CLF_ARGS.kfolds>1:
                    where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem)
                sdf = dframe[where]

                log.info("Training in mass range:%s on ntracks=%i, nevents=%i, and nfeatures=%i\n\t\tHyperparams: %r\n"%(
                    mass_tag, ntracks, sdf.shape[0], sdf.shape[1]-4, hyper_params))

                ## - - instantiate the model
                clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                    "GB%i"%hyper_params["n_estimators"],CLF_ARGS.channel, mass_tag, ntracks, CLF_ARGS.kfolds, rem, len(feats))
                sclf = SClassifier(CLF_ARGS.channel, 
                    train_df=sdf, name=clf_name, features=feats, weight_sample=False, mass_range=signal_masses, kfolds=CLF_ARGS.kfolds, fold_num=rem, **hyper_params)
                models += [sclf]

                ## - - assign the job
                if CLF_ARGS.train_bdt and CLF_ARGS.parallel:
                    jobs.append(Job(train_model, sclf, weight_sample=False, balanced=CLF_ARGS.balanced, scale_features=False, outdir=CLF_ARGS.outdir))

                if CLF_ARGS.optimize_bdt and CLF_ARGS.parallel:
                        jobs.append(Job(optimize_model, sclf, weight_sample=False, outdir=CLF_ARGS.outdir, param_grid=GB_HYPERPARAMS, balanced=CLF_ARGS.balanced))

                ## pickle the model                        
                mpath = os.path.join(CLF_ARGS.outdir, sclf.name)
                if os.path.isfile(mpath):
                    log.warning("Found %s model; skipping the preparation..."%mpath)
                    continue
                else:
                    log.info("Saving %s model to disk ..."%mpath)
                    with open(mpath, "w") as mcache:
                        cPickle.dump(sclf, mcache, protocol=2)
        else:
            for ntracks in N_TRACKS:
                for rem in range(CLF_ARGS.kfolds): #<@NOTE 1 obviously is all
                    # if CLF_ARGS.kfolds==1:
                    #     where = (dframe["tau_0_n_charged_tracks"]==ntracks)
                    # else:
                    #     where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem) & (dframe["tau_0_n_charged_tracks"]==ntracks)
                    # sdf = dframe[where]

                    if CLF_ARGS.kfolds==1:
                        where = (dframe["tau_0_n_charged_tracks"]==ntracks)
                        vdf = None
                        edf = None
                        raise Exception("Need more than 1 kfold to optimize")
                    else:
                        if CLF_ARGS.kfolds-1==(rem):
                            v_where = (dframe["event_number"]%CLF_ARGS.kfolds==0) & (dframe["tau_0_n_charged_tracks"]==ntracks)
                            where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem) & (dframe["tau_0_n_charged_tracks"]==ntracks) & (dframe["event_number"]%CLF_ARGS.kfolds!=0)
                            e_where = ((dframe["event_number"]%CLF_ARGS.kfolds==rem) & (dframe["tau_0_n_charged_tracks"]==ntracks))

                        else:
                            v_where = (dframe["event_number"]%CLF_ARGS.kfolds==(rem+1)) & (dframe["tau_0_n_charged_tracks"]==ntracks)
                            where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem) & (dframe["tau_0_n_charged_tracks"]==ntracks) & (dframe["event_number"]%CLF_ARGS.kfolds!=(rem+1))
                            e_where = ((dframe["event_number"]%CLF_ARGS.kfolds==rem) & (dframe["tau_0_n_charged_tracks"]==ntracks))

                    sdf = dframe[where]
                    vdf = dframe[v_where]
                    edf = dframe[e_where]

                    s_v_overlap = pd.merge(sdf, vdf, how="inner")
                    s_e_overlap = pd.merge(sdf, edf, how="inner")
                    v_e_overlap = pd.merge(vdf, edf, how="inner")

                    # print "Sig : Valid Overlap"
                    # print s_v_overlap
                    # print "Sig : Eval Overlap"
                    # print s_e_overlap
                    # print "Val : Eval Overlap"
                    # print v_e_overlap

                    if s_e_overlap.empty:
                        # print s_e_overlap
                        # print "No Overlap in train and evaluate"
                        pass
                    else:
                        print s_e_overlap
                        raise Excpetion("Overlap in train and evaluate")

                    if s_v_overlap.empty:
                        pass
                    else:
                        print s_v_overlap
                        raise Excpetion("Overlap in train and validation")

                    if v_e_overlap.empty:
                        pass
                    else:
                        print v_e_overlap
                        raise Excpetion("Overlap in evaluation and validation")

                               
                    log.info("Training in mass range:%s on ntracks=%i, nevents=%i, and nfeatures=%i\n\t\tHyperparams: %r\n"%(
                        mass_tag, ntracks, sdf.shape[0], sdf.shape[1]-4, hyper_params))

                    ## - - instantiate the model
                    clf_name = SClassifier.MODEL_NAME_STR_FORMAT.format(
                        "GB%i"%hyper_params["n_estimators"],CLF_ARGS.channel, mass_tag, ntracks, CLF_ARGS.kfolds, rem, len(feats))
                    sclf = SClassifier(CLF_ARGS.channel, 
                        train_df=sdf, valid_df=vdf, eval_df=edf, name=clf_name, features=feats, weight_sample=False, mass_range=signal_masses, kfolds=CLF_ARGS.kfolds, fold_num=rem, **hyper_params)
                    models += [sclf]

                    ## - - assign the job
                    if CLF_ARGS.train_bdt and CLF_ARGS.parallel:
                        jobs.append(Job(train_model, sclf, weight_sample=False, balanced=CLF_ARGS.balanced, scale_features=False, outdir=CLF_ARGS.outdir))

                    ## pickle the model                        
                    mpath = os.path.join(CLF_ARGS.outdir, sclf.name)
                    if os.path.isfile(mpath):
                        log.warning("Found %s model; skipping the preparation..."%mpath)
                        continue
                    else:
                        log.info("Saving %s model to disk ..."%mpath)
                        with open(mpath, "w") as mcache:
                            cPickle.dump(sclf, mcache, protocol=2)
                                        
    
    if CLF_ARGS.train_bdt:
        if CLF_ARGS.parallel:
            ## - - run
            log.info("**"*30 + " Submitting %i jobs "%len(jobs) + "**"*30)
            run_pool(jobs, n_jobs=-1)

        if CLF_ARGS.cluster:
            if CLF_ARGS.rs_manager=="TORQUE":
                raise RuntimeError("Not implemented yet ...")

            elif CLF_ARGS.rs_manager == "SLURM":
                # - - write jobs
                from hpana.cluster.job_template import SLURM_JOB_TEMPLATE

                # - - setup the submit dir
                os.system(
                    "mkdir -p {0}/jobs  {0}/logs {0}/models".format(CLF_ARGS.outdir))
                log.info(
                    "************** submitting %i jobs to the cluster ************" % len(models))
                log.info(
                    "**************************************************************")

                os.system("mkdir -p %s %s/jobs/" %
                        (os.path.join(CLF_ARGS.outdir, CLF_ARGS.logsdir), CLF_ARGS.outdir))
                submitfile_name = "%s/%s.sh" % (CLF_ARGS.outdir, "submitAllJobs")
                submit_file = os.open(
                    submitfile_name, os.O_CREAT | os.O_WRONLY, 0755)

                # - -  create the job scripts and run them. Returns Analysis workers
                for model in models:
                    # - - payload for container
                    payload = "source {job_script} {pickled_model} {script_path}".format(
                        job_script=os.path.join(__HERE, "slurm_job_clf.sh"),
                        pickled_model=os.path.abspath(os.path.join(CLF_ARGS.outdir, model.name)),
                        script_path=os.path.join(__HERE, script),
                    )

                    # - - write the job submission script with right permissions
                    jfile_name = "%s/jobs/%s.slurm" % (CLF_ARGS.outdir, model.name)
                    jfile = os.open(jfile_name, os.O_CREAT | os.O_WRONLY, 0755)
                    os.write(jfile,
                            SLURM_JOB_TEMPLATE.format(
                                logsdir=os.path.join(CLF_ARGS.outdir, CLF_ARGS.logsdir),
                                outdir=CLF_ARGS.outdir,
                                local_scratch=CLF_ARGS.local_scratch,
                                time="6:0:0",
                                cores=1,
                                memory="1GB",
                                jobname=model.name, 
                                payload=payload,
                                project=CLF_ARGS.rs_project)
                            )

                    os.close(jfile)

                    # - - - - write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                            (submitfile_name, model.name))
                    os.write(submit_file, "sbatch %s\n" % jfile_name)

                os.close(submit_file)
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t sh {0}".format(submitfile_name))

            elif CLF_ARGS.rs_manager=="CONDOR":
                # - - write jobs
                from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

                ## zip source code
                log.info("Creating source code tarball...")
                source_code_tarball = os.path.abspath(CLF_ARGS.outdir+"/source_code.tar.gz")
                if os.path.isfile(source_code_tarball):
                    os.system("rm -rf %s"%source_code_tarball)

                src_ds = ["bin", "hpana", "aux", "setup.sh"]
                src_ds = " ".join(src_ds)
                os.system("cd {src_dir} && tar -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar}".format(
                    src_dir=__HERE+"/../", target_tar=source_code_tarball, source_files=src_ds))

                # nodes = ["pt3wrk0", "pt3wrk1", "pt3wrk2", "pt3wrk3", "pt3wrk4"]
                # nodes = ["pt3wrk0", "pt3wrk1", "pt3wrk2", "pt3wrk4"]

                # for node in nodes:
                #     globWrkDirectory = "/nfs/work/{0}/eparrish/databank/".format(node)

                #     log.info("Creating databank directory ")
                #     os.system("if [ ! -d '%s' ]; then mkdir -p '%s'; fi" %(globWrkDirectory,globWrkDirectory)) 

                #     log.info("Copying source code tar to node")
                #     os.system("rsync -axvH --no-g --no-p %s %s; cd %s; tar -xvf source_code.tar.gz; cd -" %(source_code_tarball, globWrkDirectory, globWrkDirectory))

                #     for model in models:
                #         log.info("Transfering model file %s to %s" %(model.name,globWrkDirectory))
                #         os.system("rsync -av %s '%s'"%(os.path.join(CLF_ARGS.outdir,model.name), globWrkDirectory))

                wrkDirectory = "/disk/eparrish/databank/"

                # - - setup the submit dir
                os.system("mkdir -p {0}/jobs  {0}/logs {0}/models".format(CLF_ARGS.outdir))

                log.info("************** creating %i jobs for HTCondor submission ************" % len(models))
                log.info("**************************************************************")



                os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(CLF_ARGS.outdir))
                
                submit_file_name = "%s/submitAllJobs.sh" % (CLF_ARGS.outdir)
                submit_file = open(submit_file_name, "w")
                submit_file.write(
                             CONDOR_JOB_TEMPLATE.format(
                                 logsdir=os.path.join(
                                     CLF_ARGS.outdir, CLF_ARGS.logsdir),
                                 execScript=os.path.join(__HERE, "condor_jobs_clf.sh"),
                                 userEmail="Z1832314@students.niu.edu",
                                 memory="1GB",
                             )
                             )

                for model in models:
                    # # print "Running command: %s" %("rsync -av %s '/nfs/work/pt3wrk0/eparrish/databank/'"%(os.path.join(CLF_ARGS.outdir, model.name)))
                    # log.info("Transfering model file %s to %s" %(model.name,wrkDirectory))
                    # os.system("rsync -av %s '%s'"%(os.path.join(CLF_ARGS.outdir,model.name), wrkDirectory))

                    submit_file.write("\nArguments = %s %s %s %s %s \nqueue\n" %(   model.name, 
                                                                                    os.path.join(__HERE, "classifier-trainer"), 
                                                                                    wrkDirectory,
                                                                                    os.path.join(os.getcwd(),CLF_ARGS.outdir),
                                                                                    CLF_ARGS.balanced
                                                                                )
                    )


                    # os.close(jfile)

                    # - - - - write submission to file to be used outside container
                    log.info("Writing to %s for job %s ..." %
                             (submit_file_name, model.name))
                    # os.write(submit_file, "sbatch %s\n" % jfile_name)

                submit_file.close()
                log.warning("Submission not supported from within container.")
                log.warning("Please launch outside container via:")
                log.warning("\t condor_submit {0}".format(submit_file_name))

        # if CLF_ARGS.cluster:
        #     if CLF_ARGS.rs_manager=="CONDOR":
        #         # - - write jobs
        #         from hpana.cluster.job_template import CONDOR_JOB_TEMPLATE

        #         ## zip source code
        #         log.info("Creating source code tarball...")
        #         source_code_tarball = os.path.abspath(CLF_ARGS.outdir+"/source_code.tar.gz")
        #         if os.path.isfile(source_code_tarball):
        #             os.system("rm -rf %s"%source_code_tarball)

        #         src_ds = ["bin", "hpana", "aux", "setup.sh"]
        #         src_ds = " ".join(src_ds)
        #         os.system("cd {src_dir} && tar -cf {target_tar} {source_files} && cd - && tar --append --file={target_tar}".format(
        #             src_dir=__HERE+"/../", target_tar=source_code_tarball, source_files=src_ds))

        #         nodes = ["pt3wrk0", "pt3wrk1", "pt3wrk2", "pt3wrk3", "pt3wrk4"]
        #         # nodes = ["pt3wrk2", "pt3wrk3", "pt3wrk4"]

        #         for node in nodes:
        #             globWrkDirectory = "/nfs/work/{0}/eparrish/databank/".format(node)

        #             log.info("Creating databank directory ")
        #             os.system("if [ ! -d '%s' ]; then mkdir -p '%s'; fi" %(globWrkDirectory,globWrkDirectory)) 

        #             log.info("Copying source code tar to node")
        #             # os.system("rsync -axvH --no-g --no-p %s %s; cd %s; tar -xvf source_code.tar.gz; cd -" %(source_code_tarball, globWrkDirectory, globWrkDirectory))

        #             # for model in models:
        #                 # log.info("Transfering model file %s to %s" %(model.name,globWrkDirectory))
        #                 # os.system("rsync -av %s '%s'"%(os.path.join(CLF_ARGS.outdir,model.name), globWrkDirectory))

        #         wrkDirectory = "/disk/eparrish/databank/"

        #         # - - setup the submit dir
        #         os.system("mkdir -p {0}/jobs  {0}/logs {0}/models".format(CLF_ARGS.outdir))

        #         log.info("************** creating %i jobs for HTCondor submission ************" % len(models))
        #         log.info("**************************************************************")



        #         os.system("mkdir -p {0}/jobs  {0}/logs {0}/hists {0}/logs/log {0}/logs/err {0}/logs/out".format(CLF_ARGS.outdir))
                
        #         submit_file_name = "%s/submitAllJobs.sh" % (CLF_ARGS.outdir)
        #         submit_file = open(submit_file_name, "w")
        #         submit_file.write(
        #                      CONDOR_JOB_TEMPLATE.format(
        #                          logsdir=os.path.join(
        #                              CLF_ARGS.outdir, CLF_ARGS.logsdir),
        #                          execScript=os.path.join(__HERE, "condor_jobs_clf.sh"),
        #                          userEmail="Z1832314@students.niu.edu",
        #                          memory="2GB",
        #                      )
        #                      )

        #         for model in models:
        #             # # print "Running command: %s" %("rsync -av %s '/nfs/work/pt3wrk0/eparrish/databank/'"%(os.path.join(CLF_ARGS.outdir, model.name)))
        #             # log.info("Transfering model file %s to %s" %(model.name,wrkDirectory))
        #             # os.system("rsync -av %s '%s'"%(os.path.join(CLF_ARGS.outdir,model.name), wrkDirectory))

        #             submit_file.write("\nArguments = %s %s %s %s \nqueue\n" %(   model.name, 
        #                                                                             os.path.join(__HERE, "classifier-optimizer"), 
        #                                                                             wrkDirectory,
        #                                                                             os.path.join(os.getcwd(),CLF_ARGS.outdir)
        #                                                                         )
        #             )


        #             # os.close(jfile)

        #             # - - - - write submission to file to be used outside container
        #             log.info("Writing to %s for job %s ..." %
        #                      (submit_file_name, model.name))
        #             # os.write(submit_file, "sbatch %s\n" % jfile_name)

        #         submit_file.close()
        #         log.warning("Submission not supported from within container.")
        #         log.warning("Please launch outside container via:")
        #         log.warning("\t condor_submit {0}".format(submit_file_name))
        
##-----------------------------
## tmva backend
##-----------------------------
else:
    ## - - initialize TMVA
    ROOT.TMVA.Tools.Instance()
    ROOT.TMVA.PyMethodBase.PyInitialize()
    for signal_masses in MASS_RANGES:
        mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
        if CLF_ARGS.train_bdt:
            clf_name = "BDT_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kBDT,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel[CLF_ARGS.bin_scheme]],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
                "method_params": "!H:!V:NTrees=100:MaxDepth=10:BoostType=AdaBoost:nCuts=20",
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    bdt_classifier = Classifier(**clf_params)
                    bdt_classifier.train(backgrounds, signals, **tr_params)
            else:
                ## - - instantite classifier
                bdt_classifier = Classifier(**clf_params)
                bdt_classifier.train(backgrounds, signals, **tr_params)


        ##-----------------------------
        ## train Neural Networks
        ##-----------------------------
        if CLF_ARGS.train_nn:
            # Select Theano as backend for Keras
            environ['KERAS_BACKEND'] = 'theano'

            # Set architecture of system (AVX instruction set is not supported on SWAN)
            environ['THEANO_FLAGS'] = 'gcc.cxxflags=-march=corei7'
            from keras.models import Sequential
            from keras.layers import Dense

            clf_name = "NN_%s"%mass_tag
            clf_params = {
                "method_name": clf_name,
                "method_type": ROOT.TMVA.Types.kPyKeras,
                "output": "%s.root"%clf_name,
                "features": CLF_FEATURES[CLF_ARGS.channel[CLF_ARGS.bin_scheme]],
                "outdir": CLF_ARGS.outdir,
            }
            tr_params = {
                "signal_masses": signal_masses,
            }

            if CLF_ARGS.kfolds > 1:
                for rem in range(CLF_ARGS.kfolds):
                    model = Sequential()
                    model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                    model.add(Dense(2, activation='softmax'))
                    model.summary()
                    model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                    model_name = "%s/%s_model_mod_%i_rem_%i.h5"%(CLF_ARGS.outdir, clf_name, CLF_ARGS.kfolds, rem)
                    model.save(model_name)
                    method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                    fold_cut = ROOT.TCut(FOLD_CUT_STR.format(CLF_ARGS.kfolds, rem))
                    clf_params["method_name"] = "%s_mod_%i_rem_%i"%(clf_name, CLF_ARGS.kfolds, rem)
                    clf_params["output"] = "%s_mod_%i_rem_%i.root"%(clf_name, CLF_ARGS.kfolds, rem)

                    ## - - instantite classifier
                    nn_classifier = Classifier(**clf_params)
                    nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)
            else:
                model = Sequential()
                model.add(Dense(64, activation='relu', input_dim=len(input_features)))
                model.add(Dense(2, activation='softmax'))
                model.summary()
                model.compile(loss='categorical_crossentropy',
                              optimizer="adam", metrics=['accuracy', ])
                model_name = "%s/%s_model.h5"%(CLF_ARGS.outdir, clf_name)
                model.save(model_name)
                method_params = "H:!V:VarTransform=N:FilenameModel=%s:NumEpochs=10:BatchSize=32"%model_name

                ## - - instantite classifier
                nn_classifier = Classifier(**clf_params)
                nn_classifier.train(backgrounds, signals, method_params=method_params, **tr_params)

