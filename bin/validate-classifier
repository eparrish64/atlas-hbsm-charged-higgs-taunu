#! /usr/bin/env python 

""" Script for validating different classifier inputs
>>> Usage: validate-classifier --channel taulep --train-data TRAIN_DATA_TL_CLF_TL_26042019.pkl --plot-correlations --plot-weights --plot-sample-size  --rank-feats --outdir mva_tl_CLF_TL_26042019/ --models mva_tl_CLF_TL_26042019/model_GB200_channel_taulep_mass_*fold_0*
"""

## stdlib 
import os, time

## PyPI
import numpy as np

## - -  parse ana args (needed before ROOT)
from hpana.cmd import get_clf_parser 
clf_parser = get_clf_parser()
clf_parser.add_argument("--models", nargs="+", 
    help="pickled models")
clf_parser.add_argument("--plot-weights", action="store_true")
# clf_parser.add_argument("--plot-sample-size", action="store_true")
CLF_ARGS = clf_parser.parse_args()

## local
from hpana.config import Configuration
from hpana.analysis import Analysis
from hpana.variables import CLF_FEATURES
from hpana.categories import CLASSIFIER_CATEGORIES
from hpana.cluster.parallel import run_pool, Job
from hpana.mva.classifier import SClassifier
from hpana.mva import N_TRACKS, FOLD_CUT_STR, TRAINING_MASS_BINS, BRANCHES, ALL_FEATS
from hpana.mva.evaluation import get_models
from hpana.mva.optimization import get_hparams
from hpana.mva.validation import (
    plot_sig_dist, 
    plot_bkg_dist,
    plot_weights, 
    features_ranking, 
    features_correlation, 
    check_overfitting,
    select_features,
    explainNN) 

## - - set log level
from hpana import log
log.setLevel(CLF_ARGS.log)


# - - speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")

##-------------------------------------------------------------
## consts 
##-------------------------------------------------------------
## - - build analysis main configuration object
CONFIG = Configuration(CLF_ARGS.channel)

## - - instantiate Analysis
ANALYSIS  = Analysis(CONFIG, compile_cxx=True)

## - - training samples (no need for l-->tau fakes or MC j--> tau fakes for masses < 500GeV)
BKGS = filter(lambda b: not b.name in ["LepFakes"], ANALYSIS.backgrounds)
if CLF_ARGS.bkg:
    BKGS = filter(lambda b: b.name in CLF_ARGS.bkg, BKGS)

SIGS = ANALYSIS.get_signals()
if CLF_ARGS.sig:
    SIGS = filter(lambda s: s.name in CLF_ARGS.sig, SIGS)

## different set of bkgs for different signals regions 
TRAINING_BKGS = { 
    "LOW": filter(lambda b: not b.name in ["QCD"] , BKGS),
    "HIGH":  BKGS[:], #<! above 500 GeV
}  

## - - setup outdir
os.system("mkdir -p %s"%CLF_ARGS.outdir)

##-------------------------------------------------------------
## main driver 
##-------------------------------------------------------------
if __name__=="__main__":
    ##  prepare training Dataframe
    DFRAME = SClassifier.prepare_data(BKGS, SIGS, ALL_FEATS[CLF_ARGS.channel], data_lumi=CONFIG.data_lumi,
                                      channel=CLF_ARGS.channel, branches=BRANCHES[CLF_ARGS.channel], train_data=CLF_ARGS.train_data)

    if CLF_ARGS.plot_sample_size:
        plot_sig_dist(DFRAME, signals=SIGS, outdir=CLF_ARGS.outdir)
        plot_bkg_dist(DFRAME, backgrounds=BKGS, outdir=CLF_ARGS.outdir)

    if CLF_ARGS.plot_weights:
        plot_weights(DFRAME, outdir=CLF_ARGS.outdir) 

    ## correlation matrix ?
    if CLF_ARGS.plot_correlations :
        models = get_models(CLF_ARGS.models)
        for masses in TRAINING_MASS_BINS["NOM"]:
            mtag = "%ito%i"%(masses[0], masses[-1])
            for model in models[mtag]:
                p_title = r"Correlation Matrix: $ %i \leq m_{H^+} \leq %i [GeV]$"%(model.mass_range[0], model.mass_range[-1])
                # oname = "features_correlation_%s_channel_mass_region_%sto%s"%(CLF_ARGS.channel, signal_masses[0], signal_masses[-1])

                features_correlation(model, outdir=CLF_ARGS.outdir, title=p_title)

    ## features ranking ?
    if CLF_ARGS.rank_feats:
        # models = get_models(CLF_ARGS.models)
        # for masses in TRAINING_MASS_BINS["NOM"]:
        #     mtag = "%ito%i"%(masses[0], masses[-1])
        #     for model in models[mtag]:
        #         features_ranking(model, outdir=CLF_ARGS.outdir,)
        models, Keras_models = get_models(CLF_ARGS.models, isNN=CLF_ARGS.train_nn)
        # log.info(models)
        # log.info(Keras_models)
        model = models["80to3000"][0]
        log.info(model)
        # for sig in model.sigs:
        sig = SIGS[0]
        explainNN(model, Keras_models["80to3000"][0], backgrounds=BKGS[0], sig=sig, outdir="./", is_NN=CLF_ARGS.train_nn)

    
    # ## overfitting checks 
    # jobs = []
    # train_masses = TRAINING_MASS_BINS[CLF_ARGS.bin_scheme]
    # for signal_masses in train_masses:
    #     mass_tag = "%ito%i"%(signal_masses[0], signal_masses[-1]) if signal_masses else "ALL_MASSES"
    #     signals = filter(lambda s: s.mass in signal_masses, SIGS)
    #     p_title = r"$ %i \leq m_{H^+} \leq %i [GeV]$"%(signal_masses[0], signal_masses[-1])

    #     if CLF_ARGS.mass_range:
    #         signals = filter(lambda s:int(CLF_ARGS.mass_range[0]) <= s.mass <= int(CLF_ARGS.mass_range[1]), signals)   
    #     if not signals:
    #         log.info("No signal in %s mass range; skipping!"%mass_tag)
    #         continue

    #     ## - - different set of features for low mass and high mass
    #     if (len(signals)==1 and signals[0].mass > 400) or any([s.mass>500 for s in signals]):
    #         feats = CLF_FEATURES[CLF_ARGS.channel]["HIGH"]
    #     else:
    #         feats = CLF_FEATURES[CLF_ARGS.channel]["LOW"]
        
    #     ## get list of backgrounds to be used in this signal region 
    #     if max(signal_masses) > 400:
    #         reg_bkgs = TRAINING_BKGS["HIGH"]
    #     else:
    #         reg_bkgs = TRAINING_BKGS["LOW"]

    #     ## retrive Dataframe     
    #     dframe = DFRAME.loc[[bkg.name for bkg in reg_bkgs]+[sig.name for sig in signals]]
    #     s_dframe = DFRAME.loc[[sig.name for sig in signals]]
    #     log.info("*"*80)
    #     log.info("Signals: {0} | #events: {1} ; backgrounds: {2} | #events: {3}".format(
    #         [s.name for s in signals], s_dframe.shape[0], [b.name for b in reg_bkgs], dframe.shape[0]-s_dframe.shape[0]))
    #     log.debug(30*"*" + " Training Data Frame " + 30*"*")
    #     log.debug(dframe)
    #     for rem in range(CLF_ARGS.kfolds): 
    #         if rem!=0: #<! should be enough
    #             continue
    #         where = (dframe["event_number"]%CLF_ARGS.kfolds!=rem)
    #         sdf = dframe[where]

    #         ## tree depth checks
    #         jobs.append(Job(
    #             check_overfitting, feats, train_data=dframe, check_tree_depth=True, title=p_title, formats=["png", "eps", "pdf"],
    #             max_depths=np.linspace(1, 30, 30, endpoint=True), suffix=mass_tag+"_fold_%i"%rem, outdir=CLF_ARGS.outdir))

    #         ## min samples per split
    #         jobs.append(Job(
    #             check_overfitting, feats, train_data=dframe, check_min_samples_split=True, title=p_title, formats=["png", "eps", "pdf"],
    #              min_samples_splits=np.linspace(0.001, .1, 40, endpoint=True) , suffix=mass_tag+"_fold_%i"%rem, outdir=CLF_ARGS.outdir))

    #         ## min samples per leaf
    #         jobs.append(Job(
    #             check_overfitting, feats, train_data=dframe, check_min_samples_leaf=True, title=p_title, formats=["png", "eps", "pdf"],
    #              min_samples_leafs=np.linspace(0.001, .1, 40, endpoint=True) , suffix=mass_tag+"_fold_%i"%rem, outdir=CLF_ARGS.outdir))
            

    #         # get tuned Hyperparameters
    #         hyper_params = get_hparams(CLF_ARGS.channel, mass_range=signal_masses, bin_scheme=CLF_ARGS.bin_scheme, model_type="GB")
    #         hyper_params["verbose"] = 1

    #         ## number of Trees for boosting 
    #         jobs.append(Job(
    #             check_overfitting, feats, train_data=dframe, hyper_params=dict(hyper_params), check_num_trees=True, title=p_title, formats=["png", "eps", "pdf"],
    #              num_trees=range(40, 220, 20) , suffix=mass_tag+"_fold_%i"%rem, outdir=CLF_ARGS.outdir))

    #         ## features selection 
    #         jobs.append(Job(
    #             select_features, feats, train_data=dframe, hyper_params=dict(hyper_params), title=p_title, formats=["png", "eps", "pdf"],
    #             suffix=mass_tag+"_fold_%i"%rem, outdir=CLF_ARGS.outdir))

    # ## run jobs
    # log.info("**"*30 + " Submitting %i jobs "%len(jobs) + "**"*30)
    # run_pool(jobs, n_jobs=-1)
     

