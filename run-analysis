#! /usr/bin/env python

## stdlib
import os, sys, time, pickle, dill

# - - - - - - - -  parse ana args (needed before ROOT)
from hpana.cmd import get_ana_parser 
ana_parser = get_ana_parser()
ANA_ARGS = ana_parser.parse_args()

## local
from hpana.config import Configuration
from hpana.analysis import Analysis
from hpana.samples.sample  import Sample
from hpana.cluster.parallel import FuncWorker, run_pool, Job
from hpana import log

# - - - - time it
start_time = time.time()


# - - - - - - - -  set log level
log.setLevel(ANA_ARGS.log)

# - - - - - - - - Speed things up a bit
import ROOT
ROOT.SetSignalPolicy(ROOT.kSignalFast)
ROOT.gROOT.SetBatch(True)
log.info("ROOT is in batch mode")

# - - - - - - - - build analysis main configuration object
config = Configuration(
    ANA_ARGS.channel,
    mc_campaign=ANA_ARGS.mc_campaign,
    ntuples_version=ANA_ARGS.ntuples_version)

# - - - - - - - - some checks on cmd args
if ANA_ARGS.fields:
    fields = filter(lambda v: v.name in ANA_ARGS.fields, config.variables)
else:
    fields = config.variables
    
if ANA_ARGS.categories:
    categories = filter(lambda c: c.name in ANA_ARGS.categories, config.categories)
else:
    categories = config.categories
    
if ANA_ARGS.systematics:
    #WIP: filter based on the list of systematics available
    systematics = ANA_ARGS.systematics
else:
    systematics = ["NOMINAL"]
    
# - - - - - - - - instantiate the analysis
analysis = Analysis(config, compile_cxx=not ANA_ARGS.no_cxx)

# - - - - if you wish to look at specific samples
if ANA_ARGS.samples:
    samples = filter(lambda s: s.name in ANA_ARGS.samples, analysis.samples)
else:
    samples = analysis.samples

# - - - - if you wish to cache the fake factors
# if True:#ANA_ARGS.cache_ff:
#    analysis.cache_ffs()
   
    
# - - - - - - - - - - run on multicores
if ANA_ARGS.parallel:
    jobs = []
    for worker in analysis.workers(
            samples=samples,
            fields=fields,
            categories=categories,
            systematics=systematics):
        
        jobs.append(Job(Sample.dataset_hists, worker,
                        write_hists=True,outdir=ANA_ARGS.outdir) )

        if ANA_ARGS.dry_run:
                print "--"*100
                print worker
    if not ANA_ARGS.dry_run:   
        log.info(
            "************** submitting %i jobs  ************"%len(jobs))
        log.info(
            "***********************************************")
        run_pool(jobs, n_jobs=ANA_ARGS.ncpu)
        
# - - - - - - - - - - run on cluster
if ANA_ARGS.cluster:
    # # - - - - - - - - pickle the analysis: KEEP IT IN THE MAIN DIR, 
    # # IF YOU DONT WANT TO SHIP THE WHOLE CODE TO THE NODES.
    if ANA_ARGS.pickle_analysis:
        with open (ANA_ARGS.pickle_analysis, "wb") as pfile:
            dill.dump(analysis, pfile)
    if ANA_ARGS.pickled_analysis:
        with open (ANA_ARGS.pickled_analysis, "rb") as pfile:
            pickled_analysis = dill.load(pfile)
                    
    # - - - - clusters' resource manager and scheduler
    if ANA_ARGS.rs_manager=="TORQUE":
        # - - write jobs
        from hpana.cluster.job_template import PBS_JOB_TEMPLATE

        # - - setup the submit dir
        os.system("mkdir -p {0}/jobs {0}/failed {0}/done {0}/submitted {0}/hists".format(ANA_ARGS.outdir))
        log.info(
            "************** submitting %i jobs to the cluster ************"%len(pickled_analysis.workers))
        log.info(
            "**************************************************************")

        # - -  create the job scripts and run them.
        for aw in pickled_analysis.workers:
            jfile_name = "%s/jobs/%s.pbs"%(ANA_ARGS.outdir, aw.name)

            # - - write the job submission script with right permissions
            jfile = os.open(jfile_name, os.O_CREAT|os.O_WRONLY, 0755)
            os.write(jfile, 
                     PBS_JOB_TEMPLATE.format(logsdir=ANA_ARGS.logsdir,
                                             outdir=ANA_ARGS.outdir,
                                             local_scratch=ANA_ARGS.local_scratch,
                                             pickled_analysis=ANA_ARGS.pickled_analysis,
                                             script_path="process-dataset",
                                             jobname=aw.name,) )

            os.close(jfile)

            # - - - - submit the job now
            log.info("submitting  %s ..."%aw.name)
            if not ANA_ARGS.dry_run:
                os.system("qsub {0}".format(jfile_name))


# - - - - - - - - @NOTE: if you submitted jobs to the cluster wait till they're all done!
if ANA_ARGS.merge_hists:
    if ANA_ARGS.cluster:
        raise RuntimeError("jobs are submitted to the cluster wait for them to get done")
    log.info(
        "************** merging histograms  ************")
    log.info(
        "***********************************************")
    analysis.merge_hists(
        histsdir=ANA_ARGS.outdir,
        overwrite=True,
        samples=ANA_ARGS.samples,
        ncpu=ANA_ARGS.ncpu)

    
end_time = time.time()
elapsed_time = (end_time - start_time)/60.
log.info("\n****************** elapsed time: %0.1f mins ******************"%elapsed_time)
    
